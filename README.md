# NLPâ€‘team9 Â· MMMLU OrderÂ Sensitivity Study

> Exploring how large language models react to answerâ€‘option permutations across input/output formats and languages.

---

## ğŸ¯ Overview

This project measures **order bias** in multipleâ€‘choice question answering (MCQA) when we vary

* the **order** of answer options (circular shifts),
* the **input / output schema** (plain textÂ â‡„Â JSONÂ /Â XML), and
* the **language** of the prompt (EnglishÂ vsÂ French).

### Key research questions

| RQ | Question |
|----|-----------|
| 1Â  | Do structured formats reduce position bias compared with plain text? |
| 2Â  | Which knowledge domains are most / least robust to option permutations? |
| 3Â  | How do two public LLMs â€“Â Geminiâ€‘2.0â€‘flash andÂ Mistralâ€‘smallâ€‘latest â€“Â compare? |

---

## ğŸš€ QuickÂ start

### 1Â Â·Â Install & configure
```bash
# PythonÂ 3.10+
pip install -r requirements.txt

# .env in the project root
GOOGLE_API_KEY="yourâ€‘geminiâ€‘key"
MISTRAL_API_KEY="yourâ€‘mistralâ€‘key"
```

### 2Â Â·Â Run a small test
```bash
# Five English algebra questions, plainâ€‘text format, circular shifts
a python cli.py run \
      --subtask abstract_algebra \
      --num-questions 5 --en
```

### 3Â Â·Â Check progress
```bash
python cli.py status   # lists all running / finished experiments
```

*Experiments are resumable â€“Â rerun the same command to continue.*

---

## ğŸ“Â Project layout
```
NLPâ€‘team9/
â”œâ”€â”€ cli.py                 # unified commandâ€‘line entryâ€‘point
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ds_selected.pkl    # 17â€‘subtask MMMLU slice (EN/FR)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ single_question.py # singleâ€‘prompt pipeline
â”‚   â”œâ”€â”€ batch_runner.py    # full grid with retry & logging
â”‚   â”œâ”€â”€ format_handlers.py # 5 I/O schemas
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_permutations.py
â”‚       â””â”€â”€ test_all_formats.py
â””â”€â”€ v2_results/
    â””â”€â”€ EXP_ID/
        â”œâ”€â”€ completed/     # successful calls (.json)
        â”œâ”€â”€ pending/       # toâ€‘retry calls (.json)
        â”œâ”€â”€ status.json    # progress & statistics
        â””â”€â”€ final.jsonl    # clean output for analysis
```

---

## ğŸ“ŠÂ Experiment design

| Component   | Setting |
|-------------|---------|
| **Models**  | *Geminiâ€‘2.0â€‘flash* ([GeminiÂ 2023](https://arxiv.org/abs/2312.11805)), *Mistralâ€‘smallâ€‘latest* ([MistralÂ 7B](https://arxiv.org/abs/2310.06825)) |
| **Languages** | EnglishÂ (EN), FrenchÂ (FR) |
| **Subtasks** | 17 topics spanning STEM, humanities, social sciences, other |
| **Items** | 100 questions per subtask (test split) |
| **Permutations** | 4 circular shifts: `ABCD`, `DABC`, `CDAB`, `BCDA` |
| **I/O Schemas** | `base/base`, `base/json`, `base/xml`, `json/base`, `xml/base` |

Total calls per full grid: `2Â models Ã— 2Â langs Ã— 5Â schemas Ã— 17Â tasks Ã— 100Â q Ã— 4Â shifts = 136â€¯000`.

---

## ğŸ”§Â CLI cheatsheet

```bash
# Full EN+FR grid for a task (all five schemas)
python cli.py run --subtask abstract_algebra --format all --en --fr

# Multiple subtasks (commaâ€‘sep)
python cli.py run --subtask anatomy,astronomy,econometrics --format base/base --fr

# Factorial (24â€‘perm) deep dive on one task
python cli.py run --subtask formal_logic --permutation factorial --num-questions 20

# Check / resume
python cli.py status                # list all grids
python cli.py status EXP_ID         # details of one grid
python cli.py retry  EXP_ID         # reâ€‘issue failed calls
python cli.py reset  EXP_ID         # clear the pending queue
```

---

## ğŸ§ªÂ Testing utilities
```bash
# Verify permutation handling on a single question
python cli.py test --permutations --subtask abstract_algebra --question 0

# Smokeâ€‘test all 5 schemas
python cli.py test --formats
```

---

## âš™ï¸Â Implementation highlights

* **Retry & resume** â€“Â each prompt is stored on disk; interrupted runs pick up where they left off.
* **Format handlers** â€“Â central module converts a `Question` object to any of the five prompt schemas and parses the model response.
* **Metrics** â€“Â Fluctuationâ€¯Rate (FR) and Relativeâ€¯Standardâ€¯Deviation (RSD) computed per grid.

---

## ğŸ“ˆÂ Output schema (`final.jsonl`)
```json
{
  "question_id": "abstract_algebra_42",
  "subtask": "abstract_algebra",
  "model": "gemini-2.0-flash",
  "language": "fr",
  "input_format": "xml",
  "output_format": "base",
  "permutation": [1,2,3,0],
  "parsed_answer": "C",
  "is_correct": true,
  "timestamp": "2025-06-11 14:03:55"
}
```

---

## ğŸ™‹â€â™‚ï¸Â Troubleshooting

| Symptom | Fix |
|---------|-----|
| `Max retries exceeded` | `python cli.py reset EXP_ID` then rerun the grid |
| Blank / malformed API key | ensure `.env` has `GOOGLE_API_KEY` and/or `MISTRAL_API_KEY` |
| Want to resume after crash | just run the identical `cli.py run â€¦` command â€“Â state is persisted |

---

## ğŸ“šÂ Selected references

* HendrycksÂ etÂ al. 2021 â€“Â Measuring Massive Multitask Language Understanding.
* PezeshkpourÂ &Â HruschkaÂ 2024 â€“Â Optionâ€‘order sensitivity in LLMs.
* WeiÂ etÂ al.Â 2024 â€“Â Selection biases (order & token).
* TamÂ etÂ al.Â 2024 â€“Â Format restrictions versus freeâ€‘form generation.

_BibTeX entries are in [`references.bib`](references.bib)._  

---

## ğŸ‘¥Â Contributors

* ç›§éŸ³å­œÂ Â·Â R13922A09  
* èŠè‹±åš (Ethan)Â Â·Â R13922A24  
* å»–å‚‘æ© (Jay)Â Â·Â R13922210  


> NLPÂ (SpringÂ 2025) Â· NationalÂ TaiwanÂ University
