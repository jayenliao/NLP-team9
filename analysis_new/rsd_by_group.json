[
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8585858586,
    "accuracy_B":0.86,
    "accuracy_C":0.8585858586,
    "accuracy_D":0.8529411765,
    "avg_word_count":299.835,
    "accuracy":0.8575,
    "RSD":0.0031608624,
    "# of none":0,
    "FR":0.085,
    "FR_2":0.0025
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8256880734,
    "accuracy_B":0.8762886598,
    "accuracy_C":0.8842105263,
    "accuracy_D":0.8383838384,
    "avg_word_count":287.62,
    "accuracy":0.855,
    "RSD":0.0288276444,
    "# of none":0,
    "FR":0.095,
    "FR_2":0.0075
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8514851485,
    "accuracy_B":0.84,
    "accuracy_C":0.85,
    "accuracy_D":0.8265306122,
    "avg_word_count":224.2325,
    "accuracy":0.84,
    "RSD":0.0118358304,
    "# of none":1,
    "FR":0.1025,
    "FR_2":0.005
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8415841584,
    "accuracy_B":0.8686868687,
    "accuracy_C":0.8415841584,
    "accuracy_D":0.8571428571,
    "avg_word_count":242.4775,
    "accuracy":0.85,
    "RSD":0.013399388,
    "# of none":1,
    "FR":0.0675,
    "FR_2":0.005
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8640776699,
    "accuracy_B":0.8557692308,
    "accuracy_C":0.8947368421,
    "accuracy_D":0.887755102,
    "avg_word_count":236.6975,
    "accuracy":0.875,
    "RSD":0.0184156035,
    "# of none":0,
    "FR":0.095,
    "FR_2":0.0075
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.801980198,
    "accuracy_B":0.83,
    "accuracy_C":0.81,
    "accuracy_D":0.8282828283,
    "avg_word_count":236.04,
    "accuracy":0.8175,
    "RSD":0.0145961572,
    "# of none":0,
    "FR":0.0925,
    "FR_2":0.0075
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7941176471,
    "accuracy_B":0.8333333333,
    "accuracy_C":0.8137254902,
    "accuracy_D":0.84375,
    "avg_word_count":343.7375,
    "accuracy":0.8125,
    "RSD":0.0231442994,
    "# of none":4,
    "FR":0.1225,
    "FR_2":0.0075
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.82,
    "accuracy_B":0.8020833333,
    "accuracy_C":0.7941176471,
    "accuracy_D":0.806122449,
    "avg_word_count":312.2025,
    "accuracy":0.7975,
    "RSD":0.0116422049,
    "# of none":4,
    "FR":0.115,
    "FR_2":0.02
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.797979798,
    "accuracy_B":0.829787234,
    "accuracy_C":0.8137254902,
    "accuracy_D":0.8,
    "avg_word_count":208.9875,
    "accuracy":0.81,
    "RSD":0.0157225073,
    "# of none":0,
    "FR":0.1125,
    "FR_2":0.005
  },
  {
    "subtask":"abstract_algebra",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8095238095,
    "accuracy_B":0.8617021277,
    "accuracy_C":0.85,
    "accuracy_D":0.8217821782,
    "avg_word_count":195.0425,
    "accuracy":0.835,
    "RSD":0.0250949182,
    "# of none":0,
    "FR":0.0625,
    "FR_2":0.0025
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.73,
    "accuracy_B":0.75,
    "accuracy_C":0.75,
    "accuracy_D":0.6936936937,
    "avg_word_count":230.475,
    "accuracy":0.7275,
    "RSD":0.0314576525,
    "# of none":1,
    "FR":0.2,
    "FR_2":0.0225
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7865168539,
    "accuracy_B":0.7524752475,
    "accuracy_C":0.7474747475,
    "accuracy_D":0.7238095238,
    "avg_word_count":287.435,
    "accuracy":0.74,
    "RSD":0.0297534689,
    "# of none":6,
    "FR":0.1725,
    "FR_2":0.025
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7045454545,
    "accuracy_B":0.702970297,
    "accuracy_C":0.6732673267,
    "accuracy_D":0.6388888889,
    "avg_word_count":257.3675,
    "accuracy":0.675,
    "RSD":0.0393655803,
    "# of none":2,
    "FR":0.195,
    "FR_2":0.0225
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.693877551,
    "accuracy_B":0.7395833333,
    "accuracy_C":0.6764705882,
    "accuracy_D":0.7058823529,
    "avg_word_count":224.6575,
    "accuracy":0.7,
    "RSD":0.0327808347,
    "# of none":2,
    "FR":0.225,
    "FR_2":0.04
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7,
    "accuracy_B":0.7326732673,
    "accuracy_C":0.7448979592,
    "accuracy_D":0.7346938776,
    "avg_word_count":247.8325,
    "accuracy":0.7225,
    "RSD":0.0231483967,
    "# of none":3,
    "FR":0.215,
    "FR_2":0.0175
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7045454545,
    "accuracy_B":0.6808510638,
    "accuracy_C":0.6732673267,
    "accuracy_D":0.6272727273,
    "avg_word_count":227.655,
    "accuracy":0.6575,
    "RSD":0.0417160819,
    "# of none":7,
    "FR":0.2475,
    "FR_2":0.03
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.6666666667,
    "accuracy_B":0.637254902,
    "accuracy_C":0.6320754717,
    "accuracy_D":0.6428571429,
    "avg_word_count":289.38,
    "accuracy":0.6375,
    "RSD":0.0205296268,
    "# of none":4,
    "FR":0.225,
    "FR_2":0.0275
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.6629213483,
    "accuracy_B":0.6442307692,
    "accuracy_C":0.6095238095,
    "accuracy_D":0.62,
    "avg_word_count":300.63,
    "accuracy":0.63,
    "RSD":0.0328511225,
    "# of none":2,
    "FR":0.2175,
    "FR_2":0.035
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6734693878,
    "accuracy_B":0.65625,
    "accuracy_C":0.68,
    "accuracy_D":0.6153846154,
    "avg_word_count":255.6575,
    "accuracy":0.6525,
    "RSD":0.0383257503,
    "# of none":2,
    "FR":0.2825,
    "FR_2":0.035
  },
  {
    "subtask":"abstract_algebra",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6989247312,
    "accuracy_B":0.6979166667,
    "accuracy_C":0.6666666667,
    "accuracy_D":0.6831683168,
    "avg_word_count":242.01,
    "accuracy":0.6825,
    "RSD":0.0191139903,
    "# of none":2,
    "FR":0.2275,
    "FR_2":0.025
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.87,
    "accuracy_B":0.8736842105,
    "accuracy_C":0.86,
    "accuracy_D":0.8476190476,
    "avg_word_count":129.25,
    "accuracy":0.8625,
    "RSD":0.01171382,
    "# of none":0,
    "FR":0.095,
    "FR_2":0.0
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8198198198,
    "accuracy_B":0.8645833333,
    "accuracy_C":0.8762886598,
    "accuracy_D":0.8936170213,
    "avg_word_count":174.61,
    "accuracy":0.8575,
    "RSD":0.0316049109,
    "# of none":2,
    "FR":0.1375,
    "FR_2":0.01
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8053097345,
    "accuracy_B":0.8585858586,
    "accuracy_C":0.8988764045,
    "accuracy_D":0.8842105263,
    "avg_word_count":125.94,
    "accuracy":0.85,
    "RSD":0.0413478255,
    "# of none":4,
    "FR":0.1225,
    "FR_2":0.015
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8854166667,
    "accuracy_B":0.8979591837,
    "accuracy_C":0.8910891089,
    "accuracy_D":0.8380952381,
    "avg_word_count":100.9325,
    "accuracy":0.8775,
    "RSD":0.0268095961,
    "# of none":0,
    "FR":0.0675,
    "FR_2":0.005
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.854368932,
    "accuracy_B":0.8365384615,
    "accuracy_C":0.8673469388,
    "accuracy_D":0.8842105263,
    "avg_word_count":95.25,
    "accuracy":0.86,
    "RSD":0.0202990536,
    "# of none":0,
    "FR":0.0675,
    "FR_2":0.0
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8240740741,
    "accuracy_B":0.8541666667,
    "accuracy_C":0.8404255319,
    "accuracy_D":0.8333333333,
    "avg_word_count":121.8575,
    "accuracy":0.8375,
    "RSD":0.0131122946,
    "# of none":0,
    "FR":0.15,
    "FR_2":0.0075
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7777777778,
    "accuracy_B":0.8488372093,
    "accuracy_C":0.8888888889,
    "accuracy_D":0.8850574713,
    "avg_word_count":205.155,
    "accuracy":0.8025,
    "RSD":0.0524686999,
    "# of none":20,
    "FR":0.23,
    "FR_2":0.02
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8333333333,
    "accuracy_B":0.8735632184,
    "accuracy_C":0.8876404494,
    "accuracy_D":0.8673469388,
    "avg_word_count":207.4875,
    "accuracy":0.825,
    "RSD":0.0230604948,
    "# of none":18,
    "FR":0.2225,
    "FR_2":0.0175
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.819047619,
    "accuracy_B":0.8686868687,
    "accuracy_C":0.8645833333,
    "accuracy_D":0.82,
    "avg_word_count":91.7575,
    "accuracy":0.8425,
    "RSD":0.0279958004,
    "# of none":0,
    "FR":0.1425,
    "FR_2":0.0075
  },
  {
    "subtask":"anatomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8333333333,
    "accuracy_B":0.8556701031,
    "accuracy_C":0.8571428571,
    "accuracy_D":0.854368932,
    "avg_word_count":82.0575,
    "accuracy":0.85,
    "RSD":0.0114646187,
    "# of none":0,
    "FR":0.1075,
    "FR_2":0.0
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8085106383,
    "accuracy_B":0.7766990291,
    "accuracy_C":0.7843137255,
    "accuracy_D":0.7524752475,
    "avg_word_count":128.05,
    "accuracy":0.78,
    "RSD":0.0256163975,
    "# of none":0,
    "FR":0.14,
    "FR_2":0.015
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8125,
    "accuracy_B":0.7777777778,
    "accuracy_C":0.7755102041,
    "accuracy_D":0.8064516129,
    "avg_word_count":178.8025,
    "accuracy":0.7825,
    "RSD":0.0208988059,
    "# of none":5,
    "FR":0.1775,
    "FR_2":0.035
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8444444444,
    "accuracy_B":0.7567567568,
    "accuracy_C":0.7821782178,
    "accuracy_D":0.793814433,
    "avg_word_count":134.09,
    "accuracy":0.79,
    "RSD":0.0401646484,
    "# of none":1,
    "FR":0.1625,
    "FR_2":0.02
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7745098039,
    "accuracy_B":0.8173076923,
    "accuracy_C":0.8315789474,
    "accuracy_D":0.797979798,
    "avg_word_count":136.065,
    "accuracy":0.805,
    "RSD":0.0266054127,
    "# of none":0,
    "FR":0.15,
    "FR_2":0.01
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8172043011,
    "accuracy_B":0.8,
    "accuracy_C":0.8058252427,
    "accuracy_D":0.8181818182,
    "avg_word_count":138.485,
    "accuracy":0.81,
    "RSD":0.0094774609,
    "# of none":0,
    "FR":0.1425,
    "FR_2":0.015
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7608695652,
    "accuracy_B":0.7263157895,
    "accuracy_C":0.6785714286,
    "accuracy_D":0.6868686869,
    "avg_word_count":143.8575,
    "accuracy":0.7075,
    "RSD":0.0461714944,
    "# of none":2,
    "FR":0.2475,
    "FR_2":0.0425
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7551020408,
    "accuracy_B":0.7422680412,
    "accuracy_C":0.7115384615,
    "accuracy_D":0.7613636364,
    "avg_word_count":210.215,
    "accuracy":0.7175,
    "RSD":0.0258451701,
    "# of none":13,
    "FR":0.255,
    "FR_2":0.05
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7555555556,
    "accuracy_B":0.6542056075,
    "accuracy_C":0.6764705882,
    "accuracy_D":0.6930693069,
    "avg_word_count":205.3375,
    "accuracy":0.6925,
    "RSD":0.0542245567,
    "# of none":0,
    "FR":0.225,
    "FR_2":0.035
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7474747475,
    "accuracy_B":0.7346938776,
    "accuracy_C":0.6930693069,
    "accuracy_D":0.6960784314,
    "avg_word_count":164.435,
    "accuracy":0.7175,
    "RSD":0.0330357841,
    "# of none":0,
    "FR":0.2375,
    "FR_2":0.0275
  },
  {
    "subtask":"anatomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.72,
    "accuracy_B":0.7254901961,
    "accuracy_C":0.7244897959,
    "accuracy_D":0.7,
    "avg_word_count":157.93,
    "accuracy":0.7175,
    "RSD":0.0143697082,
    "# of none":0,
    "FR":0.24,
    "FR_2":0.025
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9387755102,
    "accuracy_B":0.931372549,
    "accuracy_C":0.9595959596,
    "accuracy_D":0.9306930693,
    "avg_word_count":147.5775,
    "accuracy":0.94,
    "RSD":0.0124333679,
    "# of none":0,
    "FR":0.0725,
    "FR_2":0.0
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9029126214,
    "accuracy_B":0.95,
    "accuracy_C":0.9381443299,
    "accuracy_D":0.9368421053,
    "avg_word_count":195.3875,
    "accuracy":0.92,
    "RSD":0.0188253397,
    "# of none":5,
    "FR":0.0925,
    "FR_2":0.025
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8932038835,
    "accuracy_B":0.9489795918,
    "accuracy_C":0.9489795918,
    "accuracy_D":0.900990099,
    "avg_word_count":136.37,
    "accuracy":0.9225,
    "RSD":0.0282620504,
    "# of none":0,
    "FR":0.075,
    "FR_2":0.015
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9473684211,
    "accuracy_B":0.9489795918,
    "accuracy_C":0.9591836735,
    "accuracy_D":0.880733945,
    "avg_word_count":95.65,
    "accuracy":0.9325,
    "RSD":0.0333199469,
    "# of none":0,
    "FR":0.06,
    "FR_2":0.0075
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9191919192,
    "accuracy_B":0.931372549,
    "accuracy_C":0.9504950495,
    "accuracy_D":0.9489795918,
    "avg_word_count":106.6775,
    "accuracy":0.9375,
    "RSD":0.0138396598,
    "# of none":0,
    "FR":0.0525,
    "FR_2":0.0
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9191919192,
    "accuracy_B":0.9583333333,
    "accuracy_C":0.932038835,
    "accuracy_D":0.9215686275,
    "avg_word_count":157.5125,
    "accuracy":0.9325,
    "RSD":0.016641585,
    "# of none":0,
    "FR":0.0825,
    "FR_2":0.0175
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9230769231,
    "accuracy_B":0.96875,
    "accuracy_C":0.9793814433,
    "accuracy_D":0.9375,
    "avg_word_count":225.9775,
    "accuracy":0.935,
    "RSD":0.0239313885,
    "# of none":7,
    "FR":0.0825,
    "FR_2":0.0175
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9292929293,
    "accuracy_B":0.9479166667,
    "accuracy_C":0.9595959596,
    "accuracy_D":0.9090909091,
    "avg_word_count":228.125,
    "accuracy":0.92,
    "RSD":0.020449539,
    "# of none":7,
    "FR":0.0875,
    "FR_2":0.015
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.900990099,
    "accuracy_B":0.9578947368,
    "accuracy_C":0.9306930693,
    "accuracy_D":0.9126213592,
    "avg_word_count":133.165,
    "accuracy":0.925,
    "RSD":0.0231915371,
    "# of none":0,
    "FR":0.08,
    "FR_2":0.0125
  },
  {
    "subtask":"astronomy",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9207920792,
    "accuracy_B":0.95,
    "accuracy_C":0.9595959596,
    "accuracy_D":0.92,
    "avg_word_count":107.9825,
    "accuracy":0.9375,
    "RSD":0.0187016352,
    "# of none":0,
    "FR":0.0725,
    "FR_2":0.005
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9347826087,
    "accuracy_B":0.8910891089,
    "accuracy_C":0.9019607843,
    "accuracy_D":0.8761904762,
    "avg_word_count":151.47,
    "accuracy":0.9,
    "RSD":0.023906872,
    "# of none":0,
    "FR":0.1025,
    "FR_2":0.0025
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8811881188,
    "accuracy_B":0.9081632653,
    "accuracy_C":0.9175257732,
    "accuracy_D":0.8712871287,
    "avg_word_count":207.115,
    "accuracy":0.8875,
    "RSD":0.0211582066,
    "# of none":3,
    "FR":0.1,
    "FR_2":0.0225
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9444444444,
    "accuracy_B":0.900990099,
    "accuracy_C":0.9038461538,
    "accuracy_D":0.8823529412,
    "avg_word_count":158.75,
    "accuracy":0.9,
    "RSD":0.0249489016,
    "# of none":3,
    "FR":0.1,
    "FR_2":0.0125
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9462365591,
    "accuracy_B":0.8823529412,
    "accuracy_C":0.91,
    "accuracy_D":0.8666666667,
    "avg_word_count":161.875,
    "accuracy":0.9,
    "RSD":0.0335308836,
    "# of none":0,
    "FR":0.085,
    "FR_2":0.01
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9462365591,
    "accuracy_B":0.91,
    "accuracy_C":0.9,
    "accuracy_D":0.8691588785,
    "avg_word_count":165.7,
    "accuracy":0.905,
    "RSD":0.0303553669,
    "# of none":0,
    "FR":0.0675,
    "FR_2":0.0075
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9347826087,
    "accuracy_B":0.9183673469,
    "accuracy_C":0.8611111111,
    "accuracy_D":0.887755102,
    "avg_word_count":179.59,
    "accuracy":0.89,
    "RSD":0.0314508509,
    "# of none":4,
    "FR":0.095,
    "FR_2":0.0175
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9072164948,
    "accuracy_B":0.9090909091,
    "accuracy_C":0.9278350515,
    "accuracy_D":0.9183673469,
    "avg_word_count":256.51,
    "accuracy":0.895,
    "RSD":0.0089729568,
    "# of none":9,
    "FR":0.095,
    "FR_2":0.01
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9468085106,
    "accuracy_B":0.9081632653,
    "accuracy_C":0.9108910891,
    "accuracy_D":0.875,
    "avg_word_count":255.495,
    "accuracy":0.9025,
    "RSD":0.027922812,
    "# of none":3,
    "FR":0.0925,
    "FR_2":0.01
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.898989899,
    "accuracy_B":0.8979591837,
    "accuracy_C":0.9191919192,
    "accuracy_D":0.8653846154,
    "avg_word_count":197.38,
    "accuracy":0.895,
    "RSD":0.0215293961,
    "# of none":0,
    "FR":0.1325,
    "FR_2":0.015
  },
  {
    "subtask":"astronomy",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9468085106,
    "accuracy_B":0.9270833333,
    "accuracy_C":0.9207920792,
    "accuracy_D":0.8425925926,
    "avg_word_count":193.6625,
    "accuracy":0.905,
    "RSD":0.0436615325,
    "# of none":1,
    "FR":0.1125,
    "FR_2":0.0075
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6910569106,
    "accuracy_B":0.8404255319,
    "accuracy_C":0.8522727273,
    "accuracy_D":0.7978723404,
    "avg_word_count":128.745,
    "accuracy":0.785,
    "RSD":0.0798986601,
    "# of none":1,
    "FR":0.1925,
    "FR_2":0.0425
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7394957983,
    "accuracy_B":0.8260869565,
    "accuracy_C":0.8421052632,
    "accuracy_D":0.8461538462,
    "avg_word_count":172.88,
    "accuracy":0.8025,
    "RSD":0.0533003914,
    "# of none":3,
    "FR":0.175,
    "FR_2":0.03
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.6846153846,
    "accuracy_B":0.8791208791,
    "accuracy_C":0.8876404494,
    "accuracy_D":0.8705882353,
    "avg_word_count":127.4525,
    "accuracy":0.805,
    "RSD":0.1016711159,
    "# of none":5,
    "FR":0.215,
    "FR_2":0.0575
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6721311475,
    "accuracy_B":0.8988764045,
    "accuracy_C":0.8777777778,
    "accuracy_D":0.8315789474,
    "avg_word_count":94.2575,
    "accuracy":0.8,
    "RSD":0.1083102368,
    "# of none":4,
    "FR":0.1375,
    "FR_2":0.0575
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7107438017,
    "accuracy_B":0.8681318681,
    "accuracy_C":0.8539325843,
    "accuracy_D":0.8229166667,
    "avg_word_count":105.155,
    "accuracy":0.8,
    "RSD":0.0759010492,
    "# of none":3,
    "FR":0.155,
    "FR_2":0.045
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7959183673,
    "accuracy_B":0.8181818182,
    "accuracy_C":0.8172043011,
    "accuracy_D":0.7735849057,
    "avg_word_count":123.8125,
    "accuracy":0.7925,
    "RSD":0.0228012524,
    "# of none":4,
    "FR":0.2275,
    "FR_2":0.0475
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7543859649,
    "accuracy_B":0.7857142857,
    "accuracy_C":0.8295454545,
    "accuracy_D":0.8351648352,
    "avg_word_count":227.6825,
    "accuracy":0.78,
    "RSD":0.0413411469,
    "# of none":9,
    "FR":0.21,
    "FR_2":0.05
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7073170732,
    "accuracy_B":0.8409090909,
    "accuracy_C":0.8020833333,
    "accuracy_D":0.8372093023,
    "avg_word_count":218.475,
    "accuracy":0.775,
    "RSD":0.0676177609,
    "# of none":7,
    "FR":0.2425,
    "FR_2":0.055
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7431192661,
    "accuracy_B":0.8666666667,
    "accuracy_C":0.8041237113,
    "accuracy_D":0.8241758242,
    "avg_word_count":124.1075,
    "accuracy":0.78,
    "RSD":0.0549628146,
    "# of none":13,
    "FR":0.205,
    "FR_2":0.0425
  },
  {
    "subtask":"business_ethics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.786407767,
    "accuracy_B":0.84375,
    "accuracy_C":0.8631578947,
    "accuracy_D":0.8586956522,
    "avg_word_count":86.5,
    "accuracy":0.8075,
    "RSD":0.0365670765,
    "# of none":14,
    "FR":0.165,
    "FR_2":0.0275
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8390804598,
    "accuracy_B":0.7916666667,
    "accuracy_C":0.77,
    "accuracy_D":0.6923076923,
    "avg_word_count":160.575,
    "accuracy":0.7675,
    "RSD":0.0685377319,
    "# of none":0,
    "FR":0.1675,
    "FR_2":0.03
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8041237113,
    "accuracy_B":0.7904761905,
    "accuracy_C":0.77,
    "accuracy_D":0.7551020408,
    "avg_word_count":211.845,
    "accuracy":0.78,
    "RSD":0.0240863618,
    "# of none":0,
    "FR":0.1625,
    "FR_2":0.02
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8202247191,
    "accuracy_B":0.797979798,
    "accuracy_C":0.7475728155,
    "accuracy_D":0.7222222222,
    "avg_word_count":149.105,
    "accuracy":0.7675,
    "RSD":0.050481128,
    "# of none":1,
    "FR":0.1625,
    "FR_2":0.0275
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7878787879,
    "accuracy_B":0.7692307692,
    "accuracy_C":0.7878787879,
    "accuracy_D":0.7551020408,
    "avg_word_count":169.29,
    "accuracy":0.775,
    "RSD":0.0177963125,
    "# of none":0,
    "FR":0.1625,
    "FR_2":0.025
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8444444444,
    "accuracy_B":0.8058252427,
    "accuracy_C":0.8144329897,
    "accuracy_D":0.7272727273,
    "avg_word_count":169.15,
    "accuracy":0.795,
    "RSD":0.0542290478,
    "# of none":0,
    "FR":0.1675,
    "FR_2":0.0175
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8533333333,
    "accuracy_B":0.7425742574,
    "accuracy_C":0.75,
    "accuracy_D":0.6271186441,
    "avg_word_count":187.015,
    "accuracy":0.72,
    "RSD":0.1077412832,
    "# of none":6,
    "FR":0.27,
    "FR_2":0.03
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7849462366,
    "accuracy_B":0.7184466019,
    "accuracy_C":0.7403846154,
    "accuracy_D":0.7,
    "avg_word_count":266.3375,
    "accuracy":0.735,
    "RSD":0.0430712738,
    "# of none":0,
    "FR":0.2225,
    "FR_2":0.03
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8292682927,
    "accuracy_B":0.7596153846,
    "accuracy_C":0.7647058824,
    "accuracy_D":0.6785714286,
    "avg_word_count":260.0025,
    "accuracy":0.7525,
    "RSD":0.070535535,
    "# of none":0,
    "FR":0.21,
    "FR_2":0.0225
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7472527473,
    "accuracy_B":0.7450980392,
    "accuracy_C":0.7272727273,
    "accuracy_D":0.6944444444,
    "avg_word_count":203.145,
    "accuracy":0.7275,
    "RSD":0.029024736,
    "# of none":0,
    "FR":0.2325,
    "FR_2":0.0375
  },
  {
    "subtask":"business_ethics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8266666667,
    "accuracy_B":0.7452830189,
    "accuracy_C":0.6893203883,
    "accuracy_D":0.6551724138,
    "avg_word_count":195.1825,
    "accuracy":0.72,
    "RSD":0.0889618475,
    "# of none":0,
    "FR":0.255,
    "FR_2":0.03
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9797979798,
    "accuracy_B":0.9898989899,
    "accuracy_C":0.97,
    "accuracy_D":0.9607843137,
    "avg_word_count":149.455,
    "accuracy":0.975,
    "RSD":0.011140273,
    "# of none":0,
    "FR":0.035,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9417475728,
    "accuracy_B":0.9494949495,
    "accuracy_C":0.9690721649,
    "accuracy_D":0.9793814433,
    "avg_word_count":205.0325,
    "accuracy":0.95,
    "RSD":0.0156386338,
    "# of none":4,
    "FR":0.0475,
    "FR_2":0.005
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9514563107,
    "accuracy_B":0.9791666667,
    "accuracy_C":0.9896907216,
    "accuracy_D":0.9696969697,
    "avg_word_count":158.6875,
    "accuracy":0.96,
    "RSD":0.0144568561,
    "# of none":5,
    "FR":0.0625,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.96,
    "accuracy_B":0.9793814433,
    "accuracy_C":0.96,
    "accuracy_D":0.9514563107,
    "avg_word_count":108.015,
    "accuracy":0.9625,
    "RSD":0.0106346219,
    "# of none":0,
    "FR":0.0425,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9795918367,
    "accuracy_B":0.9607843137,
    "accuracy_C":0.9896907216,
    "accuracy_D":0.9514563107,
    "avg_word_count":122.2625,
    "accuracy":0.97,
    "RSD":0.0155259295,
    "# of none":0,
    "FR":0.045,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9108910891,
    "accuracy_B":0.9393939394,
    "accuracy_C":0.9484536082,
    "accuracy_D":0.9223300971,
    "avg_word_count":168.405,
    "accuracy":0.93,
    "RSD":0.0156928888,
    "# of none":0,
    "FR":0.055,
    "FR_2":0.005
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9215686275,
    "accuracy_B":0.9361702128,
    "accuracy_C":0.9387755102,
    "accuracy_D":0.9375,
    "avg_word_count":243.28,
    "accuracy":0.91,
    "RSD":0.0074471634,
    "# of none":10,
    "FR":0.1025,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9108910891,
    "accuracy_B":0.9569892473,
    "accuracy_C":0.9285714286,
    "accuracy_D":0.9306930693,
    "avg_word_count":248.74,
    "accuracy":0.915,
    "RSD":0.0176617568,
    "# of none":7,
    "FR":0.105,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9207920792,
    "accuracy_B":0.9285714286,
    "accuracy_C":0.9108910891,
    "accuracy_D":0.93,
    "avg_word_count":134.4775,
    "accuracy":0.9225,
    "RSD":0.0082332747,
    "# of none":0,
    "FR":0.0625,
    "FR_2":0.005
  },
  {
    "subtask":"college_biology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9117647059,
    "accuracy_B":0.9494949495,
    "accuracy_C":0.92,
    "accuracy_D":0.9393939394,
    "avg_word_count":114.0225,
    "accuracy":0.93,
    "RSD":0.016132633,
    "# of none":0,
    "FR":0.0525,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9569892473,
    "accuracy_B":0.9108910891,
    "accuracy_C":0.9019607843,
    "accuracy_D":0.9134615385,
    "avg_word_count":151.1275,
    "accuracy":0.92,
    "RSD":0.0231432307,
    "# of none":0,
    "FR":0.075,
    "FR_2":0.0025
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9484536082,
    "accuracy_B":0.92,
    "accuracy_C":0.9029126214,
    "accuracy_D":0.91,
    "avg_word_count":209.6225,
    "accuracy":0.92,
    "RSD":0.0188284303,
    "# of none":0,
    "FR":0.0925,
    "FR_2":0.005
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9347826087,
    "accuracy_B":0.900990099,
    "accuracy_C":0.9126213592,
    "accuracy_D":0.9038461538,
    "avg_word_count":127.21,
    "accuracy":0.9125,
    "RSD":0.0145156869,
    "# of none":0,
    "FR":0.0975,
    "FR_2":0.0075
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9456521739,
    "accuracy_B":0.93,
    "accuracy_C":0.92,
    "accuracy_D":0.8796296296,
    "avg_word_count":161.0775,
    "accuracy":0.9175,
    "RSD":0.0265602217,
    "# of none":0,
    "FR":0.085,
    "FR_2":0.0
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.935483871,
    "accuracy_B":0.9108910891,
    "accuracy_C":0.9166666667,
    "accuracy_D":0.8727272727,
    "avg_word_count":162.665,
    "accuracy":0.9075,
    "RSD":0.0250846262,
    "# of none":0,
    "FR":0.11,
    "FR_2":0.005
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8787878788,
    "accuracy_B":0.898989899,
    "accuracy_C":0.8529411765,
    "accuracy_D":0.8762886598,
    "avg_word_count":180.3075,
    "accuracy":0.87,
    "RSD":0.0186182596,
    "# of none":3,
    "FR":0.135,
    "FR_2":0.005
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9042553191,
    "accuracy_B":0.8686868687,
    "accuracy_C":0.8725490196,
    "accuracy_D":0.8484848485,
    "avg_word_count":249.5025,
    "accuracy":0.86,
    "RSD":0.0228658911,
    "# of none":6,
    "FR":0.1425,
    "FR_2":0.0175
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8947368421,
    "accuracy_B":0.89,
    "accuracy_C":0.8490566038,
    "accuracy_D":0.8775510204,
    "avg_word_count":250.9675,
    "accuracy":0.875,
    "RSD":0.0202336979,
    "# of none":1,
    "FR":0.1025,
    "FR_2":0.0125
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8282828283,
    "accuracy_B":0.8380952381,
    "accuracy_C":0.8979591837,
    "accuracy_D":0.8673469388,
    "avg_word_count":196.1625,
    "accuracy":0.8575,
    "RSD":0.0317261106,
    "# of none":0,
    "FR":0.1225,
    "FR_2":0.0125
  },
  {
    "subtask":"college_biology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8876404494,
    "accuracy_B":0.8035714286,
    "accuracy_C":0.89,
    "accuracy_D":0.8484848485,
    "avg_word_count":187.4225,
    "accuracy":0.855,
    "RSD":0.0410452293,
    "# of none":0,
    "FR":0.12,
    "FR_2":0.005
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.679245283,
    "accuracy_B":0.7391304348,
    "accuracy_C":0.7087378641,
    "accuracy_D":0.7070707071,
    "avg_word_count":204.14,
    "accuracy":0.7075,
    "RSD":0.0299070495,
    "# of none":0,
    "FR":0.1325,
    "FR_2":0.0225
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7623762376,
    "accuracy_B":0.7551020408,
    "accuracy_C":0.7475728155,
    "accuracy_D":0.7448979592,
    "avg_word_count":254.045,
    "accuracy":0.7525,
    "RSD":0.0090712768,
    "# of none":0,
    "FR":0.1025,
    "FR_2":0.005
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7473684211,
    "accuracy_B":0.7142857143,
    "accuracy_C":0.7244897959,
    "accuracy_D":0.7115384615,
    "avg_word_count":191.0975,
    "accuracy":0.715,
    "RSD":0.0194642308,
    "# of none":5,
    "FR":0.1125,
    "FR_2":0.01
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7765957447,
    "accuracy_B":0.7222222222,
    "accuracy_C":0.7676767677,
    "accuracy_D":0.7676767677,
    "avg_word_count":182.625,
    "accuracy":0.7575,
    "RSD":0.0280584195,
    "# of none":0,
    "FR":0.1275,
    "FR_2":0.0125
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7281553398,
    "accuracy_B":0.7551020408,
    "accuracy_C":0.7448979592,
    "accuracy_D":0.7326732673,
    "avg_word_count":168.06,
    "accuracy":0.74,
    "RSD":0.0142632792,
    "# of none":0,
    "FR":0.105,
    "FR_2":0.0125
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7604166667,
    "accuracy_B":0.7373737374,
    "accuracy_C":0.7450980392,
    "accuracy_D":0.7184466019,
    "avg_word_count":201.465,
    "accuracy":0.74,
    "RSD":0.0204162217,
    "# of none":0,
    "FR":0.1,
    "FR_2":0.015
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7264150943,
    "accuracy_B":0.7551020408,
    "accuracy_C":0.7684210526,
    "accuracy_D":0.7422680412,
    "avg_word_count":283.6975,
    "accuracy":0.74,
    "RSD":0.0207766018,
    "# of none":4,
    "FR":0.1525,
    "FR_2":0.01
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7525773196,
    "accuracy_B":0.7142857143,
    "accuracy_C":0.7169811321,
    "accuracy_D":0.7684210526,
    "avg_word_count":288.885,
    "accuracy":0.73,
    "RSD":0.0313539742,
    "# of none":4,
    "FR":0.115,
    "FR_2":0.01
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7425742574,
    "accuracy_B":0.7628865979,
    "accuracy_C":0.7291666667,
    "accuracy_D":0.7169811321,
    "avg_word_count":177.13,
    "accuracy":0.7375,
    "RSD":0.023078595,
    "# of none":0,
    "FR":0.115,
    "FR_2":0.0075
  },
  {
    "subtask":"college_chemistry",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7352941176,
    "accuracy_B":0.73,
    "accuracy_C":0.7373737374,
    "accuracy_D":0.7448979592,
    "avg_word_count":159.235,
    "accuracy":0.735,
    "RSD":0.0072567504,
    "# of none":1,
    "FR":0.08,
    "FR_2":0.0075
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6989247312,
    "accuracy_B":0.688172043,
    "accuracy_C":0.6320754717,
    "accuracy_D":0.6261682243,
    "avg_word_count":197.98,
    "accuracy":0.6575,
    "RSD":0.0491490424,
    "# of none":1,
    "FR":0.205,
    "FR_2":0.0425
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7282608696,
    "accuracy_B":0.6767676768,
    "accuracy_C":0.641509434,
    "accuracy_D":0.6534653465,
    "avg_word_count":254.625,
    "accuracy":0.67,
    "RSD":0.0492756493,
    "# of none":2,
    "FR":0.195,
    "FR_2":0.0375
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7222222222,
    "accuracy_B":0.6868686869,
    "accuracy_C":0.6605504587,
    "accuracy_D":0.637254902,
    "avg_word_count":201.9925,
    "accuracy":0.675,
    "RSD":0.0466847664,
    "# of none":0,
    "FR":0.165,
    "FR_2":0.015
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7222222222,
    "accuracy_B":0.68,
    "accuracy_C":0.6831683168,
    "accuracy_D":0.623853211,
    "avg_word_count":194.9075,
    "accuracy":0.675,
    "RSD":0.0517607724,
    "# of none":0,
    "FR":0.19,
    "FR_2":0.035
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6914893617,
    "accuracy_B":0.6633663366,
    "accuracy_C":0.6571428571,
    "accuracy_D":0.67,
    "avg_word_count":191.6225,
    "accuracy":0.67,
    "RSD":0.0193038138,
    "# of none":0,
    "FR":0.195,
    "FR_2":0.0325
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.724137931,
    "accuracy_B":0.6836734694,
    "accuracy_C":0.6181818182,
    "accuracy_D":0.6842105263,
    "avg_word_count":227.3225,
    "accuracy":0.6575,
    "RSD":0.0560885962,
    "# of none":10,
    "FR":0.24,
    "FR_2":0.055
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.6703296703,
    "accuracy_B":0.6380952381,
    "accuracy_C":0.5909090909,
    "accuracy_D":0.6666666667,
    "avg_word_count":278.6825,
    "accuracy":0.6375,
    "RSD":0.0495135707,
    "# of none":1,
    "FR":0.245,
    "FR_2":0.05
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7261904762,
    "accuracy_B":0.5840707965,
    "accuracy_C":0.6095238095,
    "accuracy_D":0.6428571429,
    "avg_word_count":281.4425,
    "accuracy":0.635,
    "RSD":0.0836645596,
    "# of none":0,
    "FR":0.2575,
    "FR_2":0.0325
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6741573034,
    "accuracy_B":0.6808510638,
    "accuracy_C":0.5526315789,
    "accuracy_D":0.6213592233,
    "avg_word_count":225.07,
    "accuracy":0.6275,
    "RSD":0.0813283147,
    "# of none":0,
    "FR":0.2075,
    "FR_2":0.0525
  },
  {
    "subtask":"college_chemistry",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.632183908,
    "accuracy_B":0.6421052632,
    "accuracy_C":0.5625,
    "accuracy_D":0.6132075472,
    "avg_word_count":217.44,
    "accuracy":0.61,
    "RSD":0.0500857311,
    "# of none":0,
    "FR":0.2575,
    "FR_2":0.055
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9278350515,
    "accuracy_B":0.9387755102,
    "accuracy_C":0.9142857143,
    "accuracy_D":0.92,
    "avg_word_count":230.975,
    "accuracy":0.925,
    "RSD":0.0099264481,
    "# of none":0,
    "FR":0.085,
    "FR_2":0.0025
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9038461538,
    "accuracy_B":0.93,
    "accuracy_C":0.9108910891,
    "accuracy_D":0.9473684211,
    "avg_word_count":298.7575,
    "accuracy":0.9225,
    "RSD":0.0184201652,
    "# of none":0,
    "FR":0.065,
    "FR_2":0.0
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8962264151,
    "accuracy_B":0.9306930693,
    "accuracy_C":0.9578947368,
    "accuracy_D":0.9381443299,
    "avg_word_count":235.1225,
    "accuracy":0.9275,
    "RSD":0.023924686,
    "# of none":1,
    "FR":0.0875,
    "FR_2":0.0025
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9019607843,
    "accuracy_B":0.9578947368,
    "accuracy_C":0.9215686275,
    "accuracy_D":0.9108910891,
    "avg_word_count":250.4775,
    "accuracy":0.9225,
    "RSD":0.0230379282,
    "# of none":0,
    "FR":0.085,
    "FR_2":0.01
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9278350515,
    "accuracy_B":0.931372549,
    "accuracy_C":0.9393939394,
    "accuracy_D":0.9215686275,
    "avg_word_count":211.635,
    "accuracy":0.93,
    "RSD":0.0069244853,
    "# of none":0,
    "FR":0.0775,
    "FR_2":0.0025
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9072164948,
    "accuracy_B":0.9191919192,
    "accuracy_C":0.9285714286,
    "accuracy_D":0.8773584906,
    "avg_word_count":236.08,
    "accuracy":0.9075,
    "RSD":0.021239044,
    "# of none":0,
    "FR":0.0975,
    "FR_2":0.015
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8468468468,
    "accuracy_B":0.9166666667,
    "accuracy_C":0.9263157895,
    "accuracy_D":0.8842105263,
    "avg_word_count":328.025,
    "accuracy":0.885,
    "RSD":0.0348404569,
    "# of none":3,
    "FR":0.15,
    "FR_2":0.015
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7946428571,
    "accuracy_B":0.8829787234,
    "accuracy_C":0.9081632653,
    "accuracy_D":0.8631578947,
    "avg_word_count":321.4775,
    "accuracy":0.8575,
    "RSD":0.0488940624,
    "# of none":1,
    "FR":0.1375,
    "FR_2":0.02
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8910891089,
    "accuracy_B":0.898989899,
    "accuracy_C":0.91,
    "accuracy_D":0.89,
    "avg_word_count":201.6275,
    "accuracy":0.8975,
    "RSD":0.0089102797,
    "# of none":0,
    "FR":0.12,
    "FR_2":0.005
  },
  {
    "subtask":"college_computer_science",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8598130841,
    "accuracy_B":0.914893617,
    "accuracy_C":0.92,
    "accuracy_D":0.898989899,
    "avg_word_count":210.35,
    "accuracy":0.8975,
    "RSD":0.0262685414,
    "# of none":0,
    "FR":0.14,
    "FR_2":0.005
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8571428571,
    "accuracy_B":0.8181818182,
    "accuracy_C":0.7920792079,
    "accuracy_D":0.7706422018,
    "avg_word_count":213.7325,
    "accuracy":0.8075,
    "RSD":0.0398310719,
    "# of none":0,
    "FR":0.1375,
    "FR_2":0.01
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8777777778,
    "accuracy_B":0.8842105263,
    "accuracy_C":0.8333333333,
    "accuracy_D":0.724137931,
    "avg_word_count":270.4425,
    "accuracy":0.8175,
    "RSD":0.0772510594,
    "# of none":3,
    "FR":0.1675,
    "FR_2":0.0225
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8181818182,
    "accuracy_B":0.82,
    "accuracy_C":0.76,
    "accuracy_D":0.7387387387,
    "avg_word_count":222.09,
    "accuracy":0.78,
    "RSD":0.0454812866,
    "# of none":1,
    "FR":0.185,
    "FR_2":0.0125
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8020833333,
    "accuracy_B":0.797979798,
    "accuracy_C":0.8172043011,
    "accuracy_D":0.7142857143,
    "avg_word_count":217.2325,
    "accuracy":0.78,
    "RSD":0.0514116143,
    "# of none":0,
    "FR":0.175,
    "FR_2":0.0225
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8041237113,
    "accuracy_B":0.8556701031,
    "accuracy_C":0.8080808081,
    "accuracy_D":0.8,
    "avg_word_count":236.835,
    "accuracy":0.8125,
    "RSD":0.02757294,
    "# of none":2,
    "FR":0.1475,
    "FR_2":0.03
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8045977011,
    "accuracy_B":0.7835051546,
    "accuracy_C":0.7378640777,
    "accuracy_D":0.7184466019,
    "avg_word_count":235.225,
    "accuracy":0.74,
    "RSD":0.0452920731,
    "# of none":10,
    "FR":0.22,
    "FR_2":0.025
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8021978022,
    "accuracy_B":0.76,
    "accuracy_C":0.7755102041,
    "accuracy_D":0.7064220183,
    "avg_word_count":298.74,
    "accuracy":0.755,
    "RSD":0.0459317196,
    "# of none":2,
    "FR":0.205,
    "FR_2":0.04
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8,
    "accuracy_B":0.7731958763,
    "accuracy_C":0.7523809524,
    "accuracy_D":0.7142857143,
    "avg_word_count":304.465,
    "accuracy":0.755,
    "RSD":0.0412029496,
    "# of none":1,
    "FR":0.23,
    "FR_2":0.035
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7604166667,
    "accuracy_B":0.76,
    "accuracy_C":0.7684210526,
    "accuracy_D":0.7169811321,
    "avg_word_count":254.78,
    "accuracy":0.745,
    "RSD":0.0268602942,
    "# of none":3,
    "FR":0.2125,
    "FR_2":0.025
  },
  {
    "subtask":"college_computer_science",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7934782609,
    "accuracy_B":0.7788461538,
    "accuracy_C":0.7474747475,
    "accuracy_D":0.7572815534,
    "avg_word_count":240.04,
    "accuracy":0.765,
    "RSD":0.0234033009,
    "# of none":2,
    "FR":0.215,
    "FR_2":0.025
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8387096774,
    "accuracy_B":0.8155339806,
    "accuracy_C":0.8453608247,
    "accuracy_D":0.8037383178,
    "avg_word_count":228.1975,
    "accuracy":0.825,
    "RSD":0.0204547083,
    "# of none":0,
    "FR":0.135,
    "FR_2":0.0075
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.819047619,
    "accuracy_B":0.8541666667,
    "accuracy_C":0.8469387755,
    "accuracy_D":0.8282828283,
    "avg_word_count":261.32,
    "accuracy":0.8325,
    "RSD":0.0168061883,
    "# of none":2,
    "FR":0.0975,
    "FR_2":0.0025
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.793814433,
    "accuracy_B":0.8461538462,
    "accuracy_C":0.8390804598,
    "accuracy_D":0.8125,
    "avg_word_count":216.28,
    "accuracy":0.7625,
    "RSD":0.025466884,
    "# of none":29,
    "FR":0.2025,
    "FR_2":0.0075
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.801980198,
    "accuracy_B":0.8217821782,
    "accuracy_C":0.8541666667,
    "accuracy_D":0.8235294118,
    "avg_word_count":178.6425,
    "accuracy":0.825,
    "RSD":0.0226066362,
    "# of none":0,
    "FR":0.1075,
    "FR_2":0.01
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8282828283,
    "accuracy_B":0.8365384615,
    "accuracy_C":0.8723404255,
    "accuracy_D":0.8446601942,
    "avg_word_count":174.05,
    "accuracy":0.845,
    "RSD":0.019595212,
    "# of none":0,
    "FR":0.07,
    "FR_2":0.005
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.847826087,
    "accuracy_B":0.7777777778,
    "accuracy_C":0.8181818182,
    "accuracy_D":0.8217821782,
    "avg_word_count":222.055,
    "accuracy":0.815,
    "RSD":0.030692392,
    "# of none":0,
    "FR":0.105,
    "FR_2":0.0025
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.78,
    "accuracy_B":0.8265306122,
    "accuracy_C":0.8404255319,
    "accuracy_D":0.7663551402,
    "avg_word_count":287.5725,
    "accuracy":0.8,
    "RSD":0.0384978936,
    "# of none":1,
    "FR":0.0975,
    "FR_2":0.0025
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7289719626,
    "accuracy_B":0.8043478261,
    "accuracy_C":0.81,
    "accuracy_D":0.8181818182,
    "avg_word_count":284.955,
    "accuracy":0.785,
    "RSD":0.0452833236,
    "# of none":2,
    "FR":0.155,
    "FR_2":0.0175
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7857142857,
    "accuracy_B":0.7878787879,
    "accuracy_C":0.8,
    "accuracy_D":0.7961165049,
    "avg_word_count":173.6675,
    "accuracy":0.7925,
    "RSD":0.0073775122,
    "# of none":0,
    "FR":0.1025,
    "FR_2":0.005
  },
  {
    "subtask":"econometrics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7843137255,
    "accuracy_B":0.7961165049,
    "accuracy_C":0.8279569892,
    "accuracy_D":0.7843137255,
    "avg_word_count":160.4875,
    "accuracy":0.7975,
    "RSD":0.0223721429,
    "# of none":0,
    "FR":0.1375,
    "FR_2":0.0075
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.797752809,
    "accuracy_B":0.7448979592,
    "accuracy_C":0.73,
    "accuracy_D":0.6637168142,
    "avg_word_count":184.96,
    "accuracy":0.73,
    "RSD":0.0651127487,
    "# of none":0,
    "FR":0.1775,
    "FR_2":0.0275
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8021978022,
    "accuracy_B":0.7524752475,
    "accuracy_C":0.7184466019,
    "accuracy_D":0.7142857143,
    "avg_word_count":242.56,
    "accuracy":0.7325,
    "RSD":0.04715995,
    "# of none":7,
    "FR":0.2025,
    "FR_2":0.02
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8048780488,
    "accuracy_B":0.6981132075,
    "accuracy_C":0.7352941176,
    "accuracy_D":0.7247706422,
    "avg_word_count":183.695,
    "accuracy":0.735,
    "RSD":0.0532138855,
    "# of none":1,
    "FR":0.17,
    "FR_2":0.03
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7727272727,
    "accuracy_B":0.6730769231,
    "accuracy_C":0.7448979592,
    "accuracy_D":0.6727272727,
    "avg_word_count":190.3825,
    "accuracy":0.7125,
    "RSD":0.061559588,
    "# of none":0,
    "FR":0.165,
    "FR_2":0.015
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7727272727,
    "accuracy_B":0.72,
    "accuracy_C":0.7102803738,
    "accuracy_D":0.7238095238,
    "avg_word_count":189.465,
    "accuracy":0.73,
    "RSD":0.0330637765,
    "# of none":0,
    "FR":0.165,
    "FR_2":0.0125
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.724137931,
    "accuracy_B":0.65,
    "accuracy_C":0.6363636364,
    "accuracy_D":0.6355140187,
    "avg_word_count":207.555,
    "accuracy":0.6475,
    "RSD":0.055352363,
    "# of none":7,
    "FR":0.22,
    "FR_2":0.0375
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.6913580247,
    "accuracy_B":0.637254902,
    "accuracy_C":0.6095238095,
    "accuracy_D":0.640776699,
    "avg_word_count":281.165,
    "accuracy":0.6275,
    "RSD":0.0457829982,
    "# of none":9,
    "FR":0.2675,
    "FR_2":0.045
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.75,
    "accuracy_B":0.6285714286,
    "accuracy_C":0.6481481481,
    "accuracy_D":0.619047619,
    "avg_word_count":277.735,
    "accuracy":0.6525,
    "RSD":0.0789099777,
    "# of none":2,
    "FR":0.2525,
    "FR_2":0.0275
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7710843373,
    "accuracy_B":0.6388888889,
    "accuracy_C":0.6476190476,
    "accuracy_D":0.6442307692,
    "avg_word_count":225.4625,
    "accuracy":0.67,
    "RSD":0.0818689182,
    "# of none":0,
    "FR":0.2575,
    "FR_2":0.025
  },
  {
    "subtask":"econometrics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.75,
    "accuracy_B":0.6595744681,
    "accuracy_C":0.5765765766,
    "accuracy_D":0.6181818182,
    "avg_word_count":234.795,
    "accuracy":0.6425,
    "RSD":0.0986163085,
    "# of none":1,
    "FR":0.235,
    "FR_2":0.0375
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8686868687,
    "accuracy_B":0.8571428571,
    "accuracy_C":0.820754717,
    "accuracy_D":0.8969072165,
    "avg_word_count":117.2175,
    "accuracy":0.86,
    "RSD":0.0317213143,
    "# of none":0,
    "FR":0.1225,
    "FR_2":0.005
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.862745098,
    "accuracy_B":0.8181818182,
    "accuracy_C":0.8585858586,
    "accuracy_D":0.8484848485,
    "avg_word_count":176.61,
    "accuracy":0.845,
    "RSD":0.0205752499,
    "# of none":1,
    "FR":0.135,
    "FR_2":0.0125
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8411214953,
    "accuracy_B":0.8777777778,
    "accuracy_C":0.8333333333,
    "accuracy_D":0.8118811881,
    "avg_word_count":125.105,
    "accuracy":0.84,
    "RSD":0.028258312,
    "# of none":0,
    "FR":0.135,
    "FR_2":0.0025
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8645833333,
    "accuracy_B":0.8383838384,
    "accuracy_C":0.8585858586,
    "accuracy_D":0.8301886792,
    "avg_word_count":85.4625,
    "accuracy":0.8475,
    "RSD":0.0166445546,
    "# of none":0,
    "FR":0.115,
    "FR_2":0.01
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8686868687,
    "accuracy_B":0.8461538462,
    "accuracy_C":0.875,
    "accuracy_D":0.8514851485,
    "avg_word_count":113.1575,
    "accuracy":0.86,
    "RSD":0.0138050019,
    "# of none":0,
    "FR":0.11,
    "FR_2":0.0075
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8556701031,
    "accuracy_B":0.8762886598,
    "accuracy_C":0.87,
    "accuracy_D":0.8301886792,
    "avg_word_count":125.085,
    "accuracy":0.8575,
    "RSD":0.0206630866,
    "# of none":0,
    "FR":0.13,
    "FR_2":0.0325
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.854368932,
    "accuracy_B":0.8645833333,
    "accuracy_C":0.8333333333,
    "accuracy_D":0.8947368421,
    "avg_word_count":198.65,
    "accuracy":0.8525,
    "RSD":0.0256748832,
    "# of none":4,
    "FR":0.1225,
    "FR_2":0.0375
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.82,
    "accuracy_B":0.85,
    "accuracy_C":0.8181818182,
    "accuracy_D":0.86,
    "avg_word_count":204.545,
    "accuracy":0.835,
    "RSD":0.0218753034,
    "# of none":1,
    "FR":0.1525,
    "FR_2":0.0225
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8659793814,
    "accuracy_B":0.8817204301,
    "accuracy_C":0.8585858586,
    "accuracy_D":0.8018018018,
    "avg_word_count":90.9825,
    "accuracy":0.85,
    "RSD":0.0354149569,
    "# of none":0,
    "FR":0.14,
    "FR_2":0.0225
  },
  {
    "subtask":"electrical_engineering",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8673469388,
    "accuracy_B":0.8383838384,
    "accuracy_C":0.8673469388,
    "accuracy_D":0.8380952381,
    "avg_word_count":87.28,
    "accuracy":0.8525,
    "RSD":0.0170663383,
    "# of none":0,
    "FR":0.1225,
    "FR_2":0.0125
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8139534884,
    "accuracy_B":0.71,
    "accuracy_C":0.7047619048,
    "accuracy_D":0.6666666667,
    "avg_word_count":134.705,
    "accuracy":0.7175,
    "RSD":0.0754940369,
    "# of none":1,
    "FR":0.265,
    "FR_2":0.0375
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8352941176,
    "accuracy_B":0.7596153846,
    "accuracy_C":0.7843137255,
    "accuracy_D":0.7378640777,
    "avg_word_count":194.7675,
    "accuracy":0.765,
    "RSD":0.0465560253,
    "# of none":6,
    "FR":0.195,
    "FR_2":0.0275
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7777777778,
    "accuracy_B":0.7222222222,
    "accuracy_C":0.7307692308,
    "accuracy_D":0.7196261682,
    "avg_word_count":172.94,
    "accuracy":0.735,
    "RSD":0.0319425857,
    "# of none":0,
    "FR":0.2375,
    "FR_2":0.015
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7752808989,
    "accuracy_B":0.7307692308,
    "accuracy_C":0.76,
    "accuracy_D":0.7075471698,
    "avg_word_count":156.7,
    "accuracy":0.74,
    "RSD":0.0351866868,
    "# of none":1,
    "FR":0.2275,
    "FR_2":0.0225
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8192771084,
    "accuracy_B":0.7037037037,
    "accuracy_C":0.7373737374,
    "accuracy_D":0.7155963303,
    "avg_word_count":155.155,
    "accuracy":0.7375,
    "RSD":0.0606381638,
    "# of none":1,
    "FR":0.1925,
    "FR_2":0.025
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8076923077,
    "accuracy_B":0.7553191489,
    "accuracy_C":0.6944444444,
    "accuracy_D":0.6454545455,
    "avg_word_count":171.0975,
    "accuracy":0.7,
    "RSD":0.0844262352,
    "# of none":10,
    "FR":0.2725,
    "FR_2":0.0275
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7528089888,
    "accuracy_B":0.7204301075,
    "accuracy_C":0.7019230769,
    "accuracy_D":0.6634615385,
    "avg_word_count":229.1425,
    "accuracy":0.69,
    "RSD":0.0455086241,
    "# of none":10,
    "FR":0.2575,
    "FR_2":0.03
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8378378378,
    "accuracy_B":0.7047619048,
    "accuracy_C":0.6607142857,
    "accuracy_D":0.6635514019,
    "avg_word_count":236.2625,
    "accuracy":0.7025,
    "RSD":0.1005551576,
    "# of none":2,
    "FR":0.25,
    "FR_2":0.0125
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7586206897,
    "accuracy_B":0.7113402062,
    "accuracy_C":0.75,
    "accuracy_D":0.6293103448,
    "avg_word_count":182.1575,
    "accuracy":0.7075,
    "RSD":0.0717728642,
    "# of none":0,
    "FR":0.225,
    "FR_2":0.0475
  },
  {
    "subtask":"electrical_engineering",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8227848101,
    "accuracy_B":0.6826923077,
    "accuracy_C":0.6697247706,
    "accuracy_D":0.691588785,
    "avg_word_count":178.5075,
    "accuracy":0.7075,
    "RSD":0.0861463677,
    "# of none":1,
    "FR":0.2525,
    "FR_2":0.0325
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.89,
    "accuracy_B":0.8969072165,
    "accuracy_C":0.9117647059,
    "accuracy_D":0.900990099,
    "avg_word_count":288.16,
    "accuracy":0.9,
    "RSD":0.0087660276,
    "# of none":0,
    "FR":0.095,
    "FR_2":0.0075
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8585858586,
    "accuracy_B":0.887755102,
    "accuracy_C":0.8888888889,
    "accuracy_D":0.8484848485,
    "avg_word_count":315.715,
    "accuracy":0.86,
    "RSD":0.0203928177,
    "# of none":5,
    "FR":0.15,
    "FR_2":0.01
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8476190476,
    "accuracy_B":0.9042553191,
    "accuracy_C":0.9191919192,
    "accuracy_D":0.8910891089,
    "avg_word_count":255.925,
    "accuracy":0.8875,
    "RSD":0.0299818061,
    "# of none":1,
    "FR":0.13,
    "FR_2":0.025
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.902173913,
    "accuracy_B":0.8811881188,
    "accuracy_C":0.8653846154,
    "accuracy_D":0.854368932,
    "avg_word_count":242.2475,
    "accuracy":0.875,
    "RSD":0.0205244419,
    "# of none":0,
    "FR":0.13,
    "FR_2":0.0
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8529411765,
    "accuracy_B":0.9247311828,
    "accuracy_C":0.9134615385,
    "accuracy_D":0.8811881188,
    "avg_word_count":287.0075,
    "accuracy":0.8925,
    "RSD":0.0315203956,
    "# of none":0,
    "FR":0.105,
    "FR_2":0.0125
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7407407407,
    "accuracy_B":0.8617021277,
    "accuracy_C":0.8469387755,
    "accuracy_D":0.83,
    "avg_word_count":267.345,
    "accuracy":0.8175,
    "RSD":0.05736258,
    "# of none":0,
    "FR":0.2025,
    "FR_2":0.02
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7920792079,
    "accuracy_B":0.8282828283,
    "accuracy_C":0.8265306122,
    "accuracy_D":0.8541666667,
    "avg_word_count":347.5125,
    "accuracy":0.8125,
    "RSD":0.0267358785,
    "# of none":6,
    "FR":0.19,
    "FR_2":0.01
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7657657658,
    "accuracy_B":0.8804347826,
    "accuracy_C":0.8762886598,
    "accuracy_D":0.8404255319,
    "avg_word_count":323.215,
    "accuracy":0.825,
    "RSD":0.0547035059,
    "# of none":6,
    "FR":0.1825,
    "FR_2":0.02
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7570093458,
    "accuracy_B":0.8105263158,
    "accuracy_C":0.8421052632,
    "accuracy_D":0.7961165049,
    "avg_word_count":226.2025,
    "accuracy":0.8,
    "RSD":0.0381466664,
    "# of none":0,
    "FR":0.1875,
    "FR_2":0.0175
  },
  {
    "subtask":"formal_logic",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7904761905,
    "accuracy_B":0.84375,
    "accuracy_C":0.8453608247,
    "accuracy_D":0.8039215686,
    "avg_word_count":215.6125,
    "accuracy":0.82,
    "RSD":0.0294288133,
    "# of none":0,
    "FR":0.1675,
    "FR_2":0.005
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7019230769,
    "accuracy_B":0.7727272727,
    "accuracy_C":0.71,
    "accuracy_D":0.712962963,
    "avg_word_count":254.505,
    "accuracy":0.7225,
    "RSD":0.0389160018,
    "# of none":0,
    "FR":0.255,
    "FR_2":0.06
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7701149425,
    "accuracy_B":0.72,
    "accuracy_C":0.7142857143,
    "accuracy_D":0.640776699,
    "avg_word_count":315.915,
    "accuracy":0.7,
    "RSD":0.0648742856,
    "# of none":5,
    "FR":0.3025,
    "FR_2":0.055
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7126436782,
    "accuracy_B":0.7628865979,
    "accuracy_C":0.6607142857,
    "accuracy_D":0.7019230769,
    "avg_word_count":278.8125,
    "accuracy":0.7075,
    "RSD":0.0512891856,
    "# of none":0,
    "FR":0.325,
    "FR_2":0.03
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6330275229,
    "accuracy_B":0.7634408602,
    "accuracy_C":0.7113402062,
    "accuracy_D":0.6534653465,
    "avg_word_count":268.69,
    "accuracy":0.6875,
    "RSD":0.0739683712,
    "# of none":0,
    "FR":0.2825,
    "FR_2":0.05
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6568627451,
    "accuracy_B":0.75,
    "accuracy_C":0.7311827957,
    "accuracy_D":0.6725663717,
    "avg_word_count":258.74,
    "accuracy":0.7,
    "RSD":0.0553835581,
    "# of none":0,
    "FR":0.2525,
    "FR_2":0.025
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.75,
    "accuracy_B":0.7380952381,
    "accuracy_C":0.7532467532,
    "accuracy_D":0.7051282051,
    "avg_word_count":285.085,
    "accuracy":0.595,
    "RSD":0.0258416054,
    "# of none":77,
    "FR":0.29,
    "FR_2":0.0525
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7127659574,
    "accuracy_B":0.670212766,
    "accuracy_C":0.6923076923,
    "accuracy_D":0.6601941748,
    "avg_word_count":342.76,
    "accuracy":0.675,
    "RSD":0.0297278672,
    "# of none":5,
    "FR":0.29,
    "FR_2":0.0275
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.6868686869,
    "accuracy_B":0.6770833333,
    "accuracy_C":0.693877551,
    "accuracy_D":0.6320754717,
    "avg_word_count":345.6675,
    "accuracy":0.67,
    "RSD":0.0358019372,
    "# of none":1,
    "FR":0.2775,
    "FR_2":0.025
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6460176991,
    "accuracy_B":0.6597938144,
    "accuracy_C":0.7096774194,
    "accuracy_D":0.7010309278,
    "avg_word_count":309.195,
    "accuracy":0.6775,
    "RSD":0.039531889,
    "# of none":0,
    "FR":0.2825,
    "FR_2":0.04
  },
  {
    "subtask":"formal_logic",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6203703704,
    "accuracy_B":0.670212766,
    "accuracy_C":0.6761904762,
    "accuracy_D":0.7096774194,
    "avg_word_count":301.895,
    "accuracy":0.6675,
    "RSD":0.0476878164,
    "# of none":0,
    "FR":0.2525,
    "FR_2":0.0525
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6117647059,
    "accuracy_B":0.6470588235,
    "accuracy_C":0.587628866,
    "accuracy_D":0.5826086957,
    "avg_word_count":102.2625,
    "accuracy":0.605,
    "RSD":0.041961711,
    "# of none":1,
    "FR":0.3375,
    "FR_2":0.0325
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.592920354,
    "accuracy_B":0.6304347826,
    "accuracy_C":0.6095238095,
    "accuracy_D":0.6363636364,
    "avg_word_count":144.45,
    "accuracy":0.6125,
    "RSD":0.027950019,
    "# of none":2,
    "FR":0.2925,
    "FR_2":0.0175
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.5769230769,
    "accuracy_B":0.5957446809,
    "accuracy_C":0.5940594059,
    "accuracy_D":0.6,
    "avg_word_count":108.4375,
    "accuracy":0.59,
    "RSD":0.0148587453,
    "# of none":1,
    "FR":0.2975,
    "FR_2":0.03
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6144578313,
    "accuracy_B":0.5784313725,
    "accuracy_C":0.61,
    "accuracy_D":0.5964912281,
    "avg_word_count":74.33,
    "accuracy":0.5975,
    "RSD":0.0233757991,
    "# of none":1,
    "FR":0.265,
    "FR_2":0.015
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.5588235294,
    "accuracy_B":0.57,
    "accuracy_C":0.6021505376,
    "accuracy_D":0.572815534,
    "avg_word_count":79.6225,
    "accuracy":0.5725,
    "RSD":0.0277940321,
    "# of none":2,
    "FR":0.3,
    "FR_2":0.03
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6385542169,
    "accuracy_B":0.618556701,
    "accuracy_C":0.580952381,
    "accuracy_D":0.5625,
    "avg_word_count":91.035,
    "accuracy":0.5925,
    "RSD":0.0499866098,
    "# of none":3,
    "FR":0.2475,
    "FR_2":0.02
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.5641025641,
    "accuracy_B":0.595959596,
    "accuracy_C":0.6333333333,
    "accuracy_D":0.6043956044,
    "avg_word_count":179.32,
    "accuracy":0.5925,
    "RSD":0.0411522435,
    "# of none":3,
    "FR":0.3325,
    "FR_2":0.0425
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.5773195876,
    "accuracy_B":0.6,
    "accuracy_C":0.5980392157,
    "accuracy_D":0.6060606061,
    "avg_word_count":172.535,
    "accuracy":0.5925,
    "RSD":0.0181813597,
    "# of none":2,
    "FR":0.3,
    "FR_2":0.03
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6588235294,
    "accuracy_B":0.5816326531,
    "accuracy_C":0.5858585859,
    "accuracy_D":0.5849056604,
    "avg_word_count":85.6775,
    "accuracy":0.5825,
    "RSD":0.0537158803,
    "# of none":12,
    "FR":0.315,
    "FR_2":0.0375
  },
  {
    "subtask":"global_facts",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.630952381,
    "accuracy_B":0.5698924731,
    "accuracy_C":0.6170212766,
    "accuracy_D":0.59,
    "avg_word_count":68.6525,
    "accuracy":0.5575,
    "RSD":0.0393008728,
    "# of none":29,
    "FR":0.3075,
    "FR_2":0.035
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6133333333,
    "accuracy_B":0.5757575758,
    "accuracy_C":0.525,
    "accuracy_D":0.5377358491,
    "avg_word_count":137.3825,
    "accuracy":0.5575,
    "RSD":0.0613959116,
    "# of none":0,
    "FR":0.4025,
    "FR_2":0.0525
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.6533333333,
    "accuracy_B":0.5583333333,
    "accuracy_C":0.5652173913,
    "accuracy_D":0.5888888889,
    "avg_word_count":207.225,
    "accuracy":0.585,
    "RSD":0.0633815048,
    "# of none":0,
    "FR":0.355,
    "FR_2":0.0575
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.6567164179,
    "accuracy_B":0.5190839695,
    "accuracy_C":0.5272727273,
    "accuracy_D":0.5434782609,
    "avg_word_count":182.1875,
    "accuracy":0.55,
    "RSD":0.0989804019,
    "# of none":0,
    "FR":0.4075,
    "FR_2":0.045
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7,
    "accuracy_B":0.5575221239,
    "accuracy_C":0.5641025641,
    "accuracy_D":0.54,
    "avg_word_count":136.305,
    "accuracy":0.58,
    "RSD":0.1082040323,
    "# of none":0,
    "FR":0.4,
    "FR_2":0.045
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.6721311475,
    "accuracy_B":0.528,
    "accuracy_C":0.5535714286,
    "accuracy_D":0.5882352941,
    "avg_word_count":143.5625,
    "accuracy":0.5725,
    "RSD":0.0929171756,
    "# of none":0,
    "FR":0.4325,
    "FR_2":0.055
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7575757576,
    "accuracy_B":0.5666666667,
    "accuracy_C":0.525862069,
    "accuracy_D":0.5816326531,
    "avg_word_count":156.14,
    "accuracy":0.59,
    "RSD":0.146025463,
    "# of none":0,
    "FR":0.345,
    "FR_2":0.045
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.5696202532,
    "accuracy_B":0.5537190083,
    "accuracy_C":0.5700934579,
    "accuracy_D":0.5760869565,
    "avg_word_count":236.83,
    "accuracy":0.565,
    "RSD":0.0146088677,
    "# of none":1,
    "FR":0.39,
    "FR_2":0.05
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.671641791,
    "accuracy_B":0.5190839695,
    "accuracy_C":0.5727272727,
    "accuracy_D":0.5652173913,
    "avg_word_count":238.8775,
    "accuracy":0.57,
    "RSD":0.0954903298,
    "# of none":0,
    "FR":0.3625,
    "FR_2":0.045
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7205882353,
    "accuracy_B":0.5454545455,
    "accuracy_C":0.5663716814,
    "accuracy_D":0.5510204082,
    "avg_word_count":167.22,
    "accuracy":0.5825,
    "RSD":0.121537044,
    "# of none":0,
    "FR":0.36,
    "FR_2":0.0425
  },
  {
    "subtask":"global_facts",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.7066666667,
    "accuracy_B":0.5663716814,
    "accuracy_C":0.5819672131,
    "accuracy_D":0.6444444444,
    "avg_word_count":155.7575,
    "accuracy":0.615,
    "RSD":0.0888755914,
    "# of none":0,
    "FR":0.3475,
    "FR_2":0.0575
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8725490196,
    "accuracy_B":0.8775510204,
    "accuracy_C":0.8349514563,
    "accuracy_D":0.8659793814,
    "avg_word_count":149.075,
    "accuracy":0.8625,
    "RSD":0.0192060223,
    "# of none":0,
    "FR":0.0575,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8476190476,
    "accuracy_B":0.8571428571,
    "accuracy_C":0.8686868687,
    "accuracy_D":0.8659793814,
    "avg_word_count":230.0325,
    "accuracy":0.8575,
    "RSD":0.0096005051,
    "# of none":1,
    "FR":0.0625,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8446601942,
    "accuracy_B":0.8333333333,
    "accuracy_C":0.8775510204,
    "accuracy_D":0.875,
    "avg_word_count":146.935,
    "accuracy":0.855,
    "RSD":0.0222542394,
    "# of none":1,
    "FR":0.07,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8888888889,
    "accuracy_B":0.8640776699,
    "accuracy_C":0.8775510204,
    "accuracy_D":0.89,
    "avg_word_count":106.19,
    "accuracy":0.88,
    "RSD":0.011895705,
    "# of none":0,
    "FR":0.0525,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8712871287,
    "accuracy_B":0.8979591837,
    "accuracy_C":0.8686868687,
    "accuracy_D":0.8712871287,
    "avg_word_count":123.1075,
    "accuracy":0.875,
    "RSD":0.0136461208,
    "# of none":1,
    "FR":0.0625,
    "FR_2":0.0125
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8585858586,
    "accuracy_B":0.8383838384,
    "accuracy_C":0.8484848485,
    "accuracy_D":0.8349514563,
    "avg_word_count":150.9775,
    "accuracy":0.845,
    "RSD":0.0109322362,
    "# of none":0,
    "FR":0.055,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8333333333,
    "accuracy_B":0.8602150538,
    "accuracy_C":0.8645833333,
    "accuracy_D":0.8541666667,
    "avg_word_count":286.9775,
    "accuracy":0.825,
    "RSD":0.0140465693,
    "# of none":13,
    "FR":0.1125,
    "FR_2":0.0175
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8095238095,
    "accuracy_B":0.8152173913,
    "accuracy_C":0.8292682927,
    "accuracy_D":0.8131868132,
    "avg_word_count":207.515,
    "accuracy":0.755,
    "RSD":0.0091609603,
    "# of none":30,
    "FR":0.2075,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8367346939,
    "accuracy_B":0.8484848485,
    "accuracy_C":0.83,
    "accuracy_D":0.8155339806,
    "avg_word_count":112.3,
    "accuracy":0.8325,
    "RSD":0.0143031892,
    "# of none":0,
    "FR":0.0675,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_european_history",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8469387755,
    "accuracy_B":0.8514851485,
    "accuracy_C":0.8645833333,
    "accuracy_D":0.8380952381,
    "avg_word_count":100.94,
    "accuracy":0.85,
    "RSD":0.0112448808,
    "# of none":0,
    "FR":0.0875,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8453608247,
    "accuracy_B":0.84,
    "accuracy_C":0.8333333333,
    "accuracy_D":0.8217821782,
    "avg_word_count":151.525,
    "accuracy":0.835,
    "RSD":0.010537735,
    "# of none":0,
    "FR":0.08,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8777777778,
    "accuracy_B":0.8076923077,
    "accuracy_C":0.8058252427,
    "accuracy_D":0.806122449,
    "avg_word_count":228.06,
    "accuracy":0.8125,
    "RSD":0.0374258096,
    "# of none":5,
    "FR":0.1175,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8850574713,
    "accuracy_B":0.8058252427,
    "accuracy_C":0.7924528302,
    "accuracy_D":0.7596153846,
    "avg_word_count":141.53,
    "accuracy":0.8075,
    "RSD":0.0568435554,
    "# of none":0,
    "FR":0.095,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8617021277,
    "accuracy_B":0.8235294118,
    "accuracy_C":0.8484848485,
    "accuracy_D":0.819047619,
    "avg_word_count":189.695,
    "accuracy":0.8375,
    "RSD":0.0210071535,
    "# of none":0,
    "FR":0.1025,
    "FR_2":0.0125
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8367346939,
    "accuracy_B":0.8018867925,
    "accuracy_C":0.8645833333,
    "accuracy_D":0.82,
    "avg_word_count":175.785,
    "accuracy":0.83,
    "RSD":0.0277699942,
    "# of none":0,
    "FR":0.1225,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8426966292,
    "accuracy_B":0.801980198,
    "accuracy_C":0.7920792079,
    "accuracy_D":0.7843137255,
    "avg_word_count":205.5675,
    "accuracy":0.79,
    "RSD":0.0279392188,
    "# of none":7,
    "FR":0.145,
    "FR_2":0.0175
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8369565217,
    "accuracy_B":0.8125,
    "accuracy_C":0.8,
    "accuracy_D":0.81,
    "avg_word_count":307.075,
    "accuracy":0.8,
    "RSD":0.0166721245,
    "# of none":7,
    "FR":0.13,
    "FR_2":0.0125
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8275862069,
    "accuracy_B":0.78,
    "accuracy_C":0.7735849057,
    "accuracy_D":0.7663551402,
    "avg_word_count":215.78,
    "accuracy":0.785,
    "RSD":0.0304892796,
    "# of none":0,
    "FR":0.1175,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8426966292,
    "accuracy_B":0.7843137255,
    "accuracy_C":0.7924528302,
    "accuracy_D":0.786407767,
    "avg_word_count":236.6775,
    "accuracy":0.8,
    "RSD":0.0299330549,
    "# of none":0,
    "FR":0.125,
    "FR_2":0.0325
  },
  {
    "subtask":"high_school_european_history",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8764044944,
    "accuracy_B":0.8235294118,
    "accuracy_C":0.8076923077,
    "accuracy_D":0.7714285714,
    "avg_word_count":221.84,
    "accuracy":0.8175,
    "RSD":0.0460664677,
    "# of none":0,
    "FR":0.1125,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9255319149,
    "accuracy_B":0.898989899,
    "accuracy_C":0.9090909091,
    "accuracy_D":0.8333333333,
    "avg_word_count":120.7725,
    "accuracy":0.89,
    "RSD":0.0392766758,
    "# of none":0,
    "FR":0.0825,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8823529412,
    "accuracy_B":0.9,
    "accuracy_C":0.9090909091,
    "accuracy_D":0.8969072165,
    "avg_word_count":184.7025,
    "accuracy":0.8925,
    "RSD":0.0107167295,
    "# of none":2,
    "FR":0.07,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8910891089,
    "accuracy_B":0.87,
    "accuracy_C":0.8979591837,
    "accuracy_D":0.9361702128,
    "avg_word_count":141.5025,
    "accuracy":0.8825,
    "RSD":0.0265984226,
    "# of none":7,
    "FR":0.0925,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8979591837,
    "accuracy_B":0.8823529412,
    "accuracy_C":0.9278350515,
    "accuracy_D":0.8834951456,
    "avg_word_count":89.955,
    "accuracy":0.8975,
    "RSD":0.0204244285,
    "# of none":0,
    "FR":0.0575,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.914893617,
    "accuracy_B":0.9,
    "accuracy_C":0.9191919192,
    "accuracy_D":0.8598130841,
    "avg_word_count":105.1375,
    "accuracy":0.8975,
    "RSD":0.026077361,
    "# of none":0,
    "FR":0.06,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8585858586,
    "accuracy_B":0.8484848485,
    "accuracy_C":0.86,
    "accuracy_D":0.8529411765,
    "avg_word_count":121.545,
    "accuracy":0.855,
    "RSD":0.0053770558,
    "# of none":0,
    "FR":0.0775,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.7981651376,
    "accuracy_B":0.8510638298,
    "accuracy_C":0.8736842105,
    "accuracy_D":0.8723404255,
    "avg_word_count":222.2625,
    "accuracy":0.83,
    "RSD":0.0360356404,
    "# of none":8,
    "FR":0.15,
    "FR_2":0.0225
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.7830188679,
    "accuracy_B":0.8453608247,
    "accuracy_C":0.9101123596,
    "accuracy_D":0.84375,
    "avg_word_count":227.0575,
    "accuracy":0.8175,
    "RSD":0.0531590371,
    "# of none":12,
    "FR":0.1875,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8829787234,
    "accuracy_B":0.8431372549,
    "accuracy_C":0.854368932,
    "accuracy_D":0.8415841584,
    "avg_word_count":114.3625,
    "accuracy":0.855,
    "RSD":0.0194088031,
    "# of none":0,
    "FR":0.11,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_geography",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8686868687,
    "accuracy_B":0.86,
    "accuracy_C":0.8686868687,
    "accuracy_D":0.8529411765,
    "avg_word_count":94.9775,
    "accuracy":0.8625,
    "RSD":0.0076495212,
    "# of none":0,
    "FR":0.08,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9166666667,
    "accuracy_B":0.8888888889,
    "accuracy_C":0.875,
    "accuracy_D":0.7876106195,
    "avg_word_count":152.245,
    "accuracy":0.8625,
    "RSD":0.0556499563,
    "# of none":0,
    "FR":0.1325,
    "FR_2":0.015
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9,
    "accuracy_B":0.9052631579,
    "accuracy_C":0.8476190476,
    "accuracy_D":0.8256880734,
    "avg_word_count":208.605,
    "accuracy":0.865,
    "RSD":0.0390264219,
    "# of none":1,
    "FR":0.1475,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.875,
    "accuracy_B":0.8317757009,
    "accuracy_C":0.8686868687,
    "accuracy_D":0.8380952381,
    "avg_word_count":132.605,
    "accuracy":0.85,
    "RSD":0.0219387204,
    "# of none":1,
    "FR":0.1575,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9277108434,
    "accuracy_B":0.8476190476,
    "accuracy_C":0.820754717,
    "accuracy_D":0.8490566038,
    "avg_word_count":159.485,
    "accuracy":0.8575,
    "RSD":0.0464109624,
    "# of none":0,
    "FR":0.1575,
    "FR_2":0.02
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8977272727,
    "accuracy_B":0.8490566038,
    "accuracy_C":0.8469387755,
    "accuracy_D":0.8425925926,
    "avg_word_count":162.015,
    "accuracy":0.8575,
    "RSD":0.0261152292,
    "# of none":0,
    "FR":0.125,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8720930233,
    "accuracy_B":0.824742268,
    "accuracy_C":0.8037383178,
    "accuracy_D":0.7706422018,
    "avg_word_count":182.095,
    "accuracy":0.8125,
    "RSD":0.04500087,
    "# of none":1,
    "FR":0.21,
    "FR_2":0.02
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8295454545,
    "accuracy_B":0.8282828283,
    "accuracy_C":0.8137254902,
    "accuracy_D":0.785046729,
    "avg_word_count":247.225,
    "accuracy":0.805,
    "RSD":0.0220058143,
    "# of none":4,
    "FR":0.1875,
    "FR_2":0.03
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8974358974,
    "accuracy_B":0.7843137255,
    "accuracy_C":0.7614678899,
    "accuracy_D":0.7567567568,
    "avg_word_count":247.7425,
    "accuracy":0.7925,
    "RSD":0.0715201166,
    "# of none":0,
    "FR":0.1925,
    "FR_2":0.0175
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8651685393,
    "accuracy_B":0.8076923077,
    "accuracy_C":0.8282828283,
    "accuracy_D":0.7777777778,
    "avg_word_count":192.2825,
    "accuracy":0.8175,
    "RSD":0.0387824374,
    "# of none":0,
    "FR":0.2,
    "FR_2":0.0125
  },
  {
    "subtask":"high_school_geography",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8651685393,
    "accuracy_B":0.84375,
    "accuracy_C":0.8137254902,
    "accuracy_D":0.7433628319,
    "avg_word_count":182.6825,
    "accuracy":0.8125,
    "RSD":0.0563509959,
    "# of none":0,
    "FR":0.2,
    "FR_2":0.02
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9797979798,
    "accuracy_B":0.9896907216,
    "accuracy_C":0.9702970297,
    "accuracy_D":0.9514563107,
    "avg_word_count":133.725,
    "accuracy":0.9725,
    "RSD":0.0145017916,
    "# of none":0,
    "FR":0.03,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9411764706,
    "accuracy_B":0.9797979798,
    "accuracy_C":0.9795918367,
    "accuracy_D":0.9603960396,
    "avg_word_count":211.7325,
    "accuracy":0.965,
    "RSD":0.0165472369,
    "# of none":0,
    "FR":0.0275,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.98,
    "accuracy_B":0.9898989899,
    "accuracy_C":0.9795918367,
    "accuracy_D":0.9607843137,
    "avg_word_count":155.7875,
    "accuracy":0.975,
    "RSD":0.0107744419,
    "# of none":1,
    "FR":0.03,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9595959596,
    "accuracy_B":0.9897959184,
    "accuracy_C":0.9607843137,
    "accuracy_D":0.9603960396,
    "avg_word_count":98.785,
    "accuracy":0.9675,
    "RSD":0.0132250575,
    "# of none":0,
    "FR":0.0475,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9898989899,
    "accuracy_B":0.9898989899,
    "accuracy_C":0.9801980198,
    "accuracy_D":0.9702970297,
    "avg_word_count":116.5525,
    "accuracy":0.9825,
    "RSD":0.008263116,
    "# of none":0,
    "FR":0.0175,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9789473684,
    "accuracy_B":0.9595959596,
    "accuracy_C":0.932038835,
    "accuracy_D":0.9223300971,
    "avg_word_count":137.6875,
    "accuracy":0.9475,
    "RSD":0.0236151248,
    "# of none":0,
    "FR":0.0625,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9504950495,
    "accuracy_B":0.9680851064,
    "accuracy_C":0.9484536082,
    "accuracy_D":0.93,
    "avg_word_count":257.305,
    "accuracy":0.93,
    "RSD":0.0142070652,
    "# of none":8,
    "FR":0.09,
    "FR_2":0.0125
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.94,
    "accuracy_B":0.9375,
    "accuracy_C":0.9591836735,
    "accuracy_D":0.9484536082,
    "avg_word_count":260.5925,
    "accuracy":0.925,
    "RSD":0.0089631491,
    "# of none":9,
    "FR":0.0925,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9387755102,
    "accuracy_B":0.9583333333,
    "accuracy_C":0.95,
    "accuracy_D":0.9245283019,
    "avg_word_count":122.175,
    "accuracy":0.9425,
    "RSD":0.0134477259,
    "# of none":0,
    "FR":0.0625,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9142857143,
    "accuracy_B":0.96875,
    "accuracy_C":0.9690721649,
    "accuracy_D":0.9509803922,
    "avg_word_count":107.4375,
    "accuracy":0.95,
    "RSD":0.0234560197,
    "# of none":0,
    "FR":0.06,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9791666667,
    "accuracy_B":0.9705882353,
    "accuracy_C":0.98,
    "accuracy_D":0.9705882353,
    "avg_word_count":160.7575,
    "accuracy":0.975,
    "RSD":0.0046223513,
    "# of none":0,
    "FR":0.04,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9693877551,
    "accuracy_B":0.9611650485,
    "accuracy_C":0.9696969697,
    "accuracy_D":0.96,
    "avg_word_count":223.5625,
    "accuracy":0.965,
    "RSD":0.0046630597,
    "# of none":0,
    "FR":0.0525,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9583333333,
    "accuracy_B":0.9405940594,
    "accuracy_C":0.9514563107,
    "accuracy_D":0.97,
    "avg_word_count":150.28,
    "accuracy":0.955,
    "RSD":0.0111810636,
    "# of none":0,
    "FR":0.045,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9787234043,
    "accuracy_B":0.9405940594,
    "accuracy_C":0.9405940594,
    "accuracy_D":0.9230769231,
    "avg_word_count":171.5925,
    "accuracy":0.945,
    "RSD":0.0215043293,
    "# of none":0,
    "FR":0.0625,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9791666667,
    "accuracy_B":0.9693877551,
    "accuracy_C":0.9326923077,
    "accuracy_D":0.9607843137,
    "avg_word_count":175.565,
    "accuracy":0.96,
    "RSD":0.0180385552,
    "# of none":0,
    "FR":0.055,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9042553191,
    "accuracy_B":0.9183673469,
    "accuracy_C":0.8811881188,
    "accuracy_D":0.8857142857,
    "avg_word_count":193.2775,
    "accuracy":0.8925,
    "RSD":0.0165848292,
    "# of none":2,
    "FR":0.105,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8888888889,
    "accuracy_B":0.9090909091,
    "accuracy_C":0.8834951456,
    "accuracy_D":0.9270833333,
    "avg_word_count":278.395,
    "accuracy":0.895,
    "RSD":0.0191489623,
    "# of none":3,
    "FR":0.1125,
    "FR_2":0.0125
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9647058824,
    "accuracy_B":0.9019607843,
    "accuracy_C":0.8598130841,
    "accuracy_D":0.8737864078,
    "avg_word_count":278.0625,
    "accuracy":0.89,
    "RSD":0.0447622624,
    "# of none":3,
    "FR":0.125,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8958333333,
    "accuracy_B":0.8775510204,
    "accuracy_C":0.8640776699,
    "accuracy_D":0.8737864078,
    "avg_word_count":218.9425,
    "accuracy":0.8775,
    "RSD":0.0131090939,
    "# of none":0,
    "FR":0.105,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_government_and_politics",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8653846154,
    "accuracy_B":0.898989899,
    "accuracy_C":0.88,
    "accuracy_D":0.9072164948,
    "avg_word_count":210.895,
    "accuracy":0.8875,
    "RSD":0.0183813208,
    "# of none":0,
    "FR":0.1025,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9897959184,
    "accuracy_B":0.9801980198,
    "accuracy_C":0.99,
    "accuracy_D":0.9801980198,
    "avg_word_count":138.9775,
    "accuracy":0.985,
    "RSD":0.0049241321,
    "# of none":0,
    "FR":0.0075,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9898989899,
    "accuracy_B":0.9504950495,
    "accuracy_C":0.9897959184,
    "accuracy_D":1.0,
    "avg_word_count":192.825,
    "accuracy":0.97,
    "RSD":0.0193008491,
    "# of none":5,
    "FR":0.05,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9801980198,
    "accuracy_B":0.9791666667,
    "accuracy_C":0.9897959184,
    "accuracy_D":0.9793814433,
    "avg_word_count":134.5675,
    "accuracy":0.9625,
    "RSD":0.0045201946,
    "# of none":8,
    "FR":0.05,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9797979798,
    "accuracy_B":0.9705882353,
    "accuracy_C":1.0,
    "accuracy_D":0.98,
    "avg_word_count":94.5675,
    "accuracy":0.9825,
    "RSD":0.0109333426,
    "# of none":0,
    "FR":0.02,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9898989899,
    "accuracy_B":0.9801980198,
    "accuracy_C":0.99,
    "accuracy_D":0.99,
    "avg_word_count":110.965,
    "accuracy":0.9875,
    "RSD":0.0042834425,
    "# of none":0,
    "FR":0.005,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.95,
    "accuracy_B":0.9693877551,
    "accuracy_C":0.95,
    "accuracy_D":0.9509803922,
    "avg_word_count":130.285,
    "accuracy":0.955,
    "RSD":0.0086518736,
    "# of none":0,
    "FR":0.065,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.95,
    "accuracy_B":0.9468085106,
    "accuracy_C":0.9578947368,
    "accuracy_D":0.9583333333,
    "avg_word_count":257.95,
    "accuracy":0.9175,
    "RSD":0.0052312139,
    "# of none":15,
    "FR":0.11,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9417475728,
    "accuracy_B":0.9569892473,
    "accuracy_C":0.9775280899,
    "accuracy_D":0.967032967,
    "avg_word_count":241.335,
    "accuracy":0.9025,
    "RSD":0.0137306427,
    "# of none":24,
    "FR":0.1325,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.96,
    "accuracy_B":0.9892473118,
    "accuracy_C":0.9326923077,
    "accuracy_D":0.931372549,
    "avg_word_count":105.955,
    "accuracy":0.95,
    "RSD":0.0248371755,
    "# of none":1,
    "FR":0.055,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_psychology",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9595959596,
    "accuracy_B":0.9587628866,
    "accuracy_C":0.95,
    "accuracy_D":0.931372549,
    "avg_word_count":96.94,
    "accuracy":0.945,
    "RSD":0.0119545243,
    "# of none":2,
    "FR":0.045,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9578947368,
    "accuracy_B":0.9791666667,
    "accuracy_C":0.9897959184,
    "accuracy_D":0.8918918919,
    "avg_word_count":148.4075,
    "accuracy":0.9525,
    "RSD":0.039836003,
    "# of none":0,
    "FR":0.06,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9387755102,
    "accuracy_B":0.9690721649,
    "accuracy_C":0.9417475728,
    "accuracy_D":0.9306930693,
    "avg_word_count":206.8525,
    "accuracy":0.9425,
    "RSD":0.0152738048,
    "# of none":1,
    "FR":0.0825,
    "FR_2":0.0025
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9696969697,
    "accuracy_B":0.9603960396,
    "accuracy_C":0.9696969697,
    "accuracy_D":0.9504950495,
    "avg_word_count":131.07,
    "accuracy":0.9625,
    "RSD":0.0082478162,
    "# of none":0,
    "FR":0.025,
    "FR_2":0.0
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9893617021,
    "accuracy_B":0.9690721649,
    "accuracy_C":0.9509803922,
    "accuracy_D":0.9065420561,
    "avg_word_count":159.42,
    "accuracy":0.9525,
    "RSD":0.0320482259,
    "# of none":0,
    "FR":0.0375,
    "FR_2":0.0075
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9894736842,
    "accuracy_B":0.9489795918,
    "accuracy_C":0.9405940594,
    "accuracy_D":0.9245283019,
    "avg_word_count":160.295,
    "accuracy":0.95,
    "RSD":0.0251807103,
    "# of none":0,
    "FR":0.05,
    "FR_2":0.005
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9139784946,
    "accuracy_B":0.8725490196,
    "accuracy_C":0.9,
    "accuracy_D":0.8659793814,
    "avg_word_count":178.8775,
    "accuracy":0.87,
    "RSD":0.0221106644,
    "# of none":8,
    "FR":0.115,
    "FR_2":0.0175
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.89,
    "accuracy_B":0.887755102,
    "accuracy_C":0.89,
    "accuracy_D":0.9081632653,
    "avg_word_count":257.4075,
    "accuracy":0.885,
    "RSD":0.0092172936,
    "# of none":4,
    "FR":0.1,
    "FR_2":0.0175
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9247311828,
    "accuracy_B":0.9072164948,
    "accuracy_C":0.887755102,
    "accuracy_D":0.8272727273,
    "avg_word_count":256.1025,
    "accuracy":0.88,
    "RSD":0.0414351423,
    "# of none":2,
    "FR":0.0975,
    "FR_2":0.0175
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8979591837,
    "accuracy_B":0.9139784946,
    "accuracy_C":0.8823529412,
    "accuracy_D":0.8691588785,
    "avg_word_count":200.045,
    "accuracy":0.89,
    "RSD":0.0188515484,
    "# of none":0,
    "FR":0.0925,
    "FR_2":0.01
  },
  {
    "subtask":"high_school_psychology",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9255319149,
    "accuracy_B":0.8490566038,
    "accuracy_C":0.900990099,
    "accuracy_D":0.8787878788,
    "avg_word_count":188.655,
    "accuracy":0.8875,
    "RSD":0.0317180418,
    "# of none":0,
    "FR":0.0725,
    "FR_2":0.005
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9255319149,
    "accuracy_B":0.8969072165,
    "accuracy_C":0.86,
    "accuracy_D":0.8623853211,
    "avg_word_count":129.8125,
    "accuracy":0.885,
    "RSD":0.0304628925,
    "# of none":0,
    "FR":0.1,
    "FR_2":0.005
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8761904762,
    "accuracy_B":0.9130434783,
    "accuracy_C":0.9239130435,
    "accuracy_D":0.900990099,
    "avg_word_count":167.6875,
    "accuracy":0.88,
    "RSD":0.019642156,
    "# of none":10,
    "FR":0.1225,
    "FR_2":0.0175
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8611111111,
    "accuracy_B":0.9444444444,
    "accuracy_C":0.8817204301,
    "accuracy_D":0.8910891089,
    "avg_word_count":130.865,
    "accuracy":0.875,
    "RSD":0.034382025,
    "# of none":8,
    "FR":0.125,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9239130435,
    "accuracy_B":0.898989899,
    "accuracy_C":0.8725490196,
    "accuracy_D":0.8504672897,
    "avg_word_count":81.545,
    "accuracy":0.885,
    "RSD":0.0311429515,
    "# of none":0,
    "FR":0.0875,
    "FR_2":0.0
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.90625,
    "accuracy_B":0.8947368421,
    "accuracy_C":0.87,
    "accuracy_D":0.8532110092,
    "avg_word_count":95.0175,
    "accuracy":0.88,
    "RSD":0.0235325221,
    "# of none":0,
    "FR":0.0875,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8865979381,
    "accuracy_B":0.887755102,
    "accuracy_C":0.8762886598,
    "accuracy_D":0.8333333333,
    "avg_word_count":142.8,
    "accuracy":0.87,
    "RSD":0.0254843562,
    "# of none":0,
    "FR":0.1025,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8349514563,
    "accuracy_B":0.8977272727,
    "accuracy_C":0.8673469388,
    "accuracy_D":0.8585858586,
    "avg_word_count":207.885,
    "accuracy":0.8375,
    "RSD":0.0259908656,
    "# of none":12,
    "FR":0.1725,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8653846154,
    "accuracy_B":0.9204545455,
    "accuracy_C":0.8829787234,
    "accuracy_D":0.8469387755,
    "avg_word_count":208.5075,
    "accuracy":0.8425,
    "RSD":0.0308848244,
    "# of none":16,
    "FR":0.145,
    "FR_2":0.0175
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.88,
    "accuracy_B":0.8854166667,
    "accuracy_C":0.875,
    "accuracy_D":0.8240740741,
    "avg_word_count":106.165,
    "accuracy":0.865,
    "RSD":0.0283501221,
    "# of none":0,
    "FR":0.12,
    "FR_2":0.005
  },
  {
    "subtask":"human_sexuality",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8686868687,
    "accuracy_B":0.8469387755,
    "accuracy_C":0.8367346939,
    "accuracy_D":0.8285714286,
    "avg_word_count":80.2675,
    "accuracy":0.845,
    "RSD":0.0177743958,
    "# of none":0,
    "FR":0.1175,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8571428571,
    "accuracy_B":0.8775510204,
    "accuracy_C":0.8113207547,
    "accuracy_D":0.8557692308,
    "avg_word_count":139.4775,
    "accuracy":0.8475,
    "RSD":0.0284319107,
    "# of none":1,
    "FR":0.1525,
    "FR_2":0.0175
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.89,
    "accuracy_B":0.8854166667,
    "accuracy_C":0.89,
    "accuracy_D":0.9157894737,
    "avg_word_count":193.9625,
    "accuracy":0.875,
    "RSD":0.0133762715,
    "# of none":9,
    "FR":0.13,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9213483146,
    "accuracy_B":0.8712871287,
    "accuracy_C":0.8285714286,
    "accuracy_D":0.8910891089,
    "avg_word_count":132.195,
    "accuracy":0.8675,
    "RSD":0.0383619846,
    "# of none":4,
    "FR":0.135,
    "FR_2":0.0075
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8888888889,
    "accuracy_B":0.8316831683,
    "accuracy_C":0.8529411765,
    "accuracy_D":0.8504672897,
    "avg_word_count":149.32,
    "accuracy":0.855,
    "RSD":0.0241752862,
    "# of none":0,
    "FR":0.135,
    "FR_2":0.0125
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9111111111,
    "accuracy_B":0.8431372549,
    "accuracy_C":0.8725490196,
    "accuracy_D":0.8679245283,
    "avg_word_count":153.5825,
    "accuracy":0.8725,
    "RSD":0.0278509806,
    "# of none":0,
    "FR":0.14,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8709677419,
    "accuracy_B":0.8686868687,
    "accuracy_C":0.8514851485,
    "accuracy_D":0.8113207547,
    "avg_word_count":165.505,
    "accuracy":0.8475,
    "RSD":0.0281021178,
    "# of none":1,
    "FR":0.175,
    "FR_2":0.015
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.806122449,
    "accuracy_B":0.8383838384,
    "accuracy_C":0.8842105263,
    "accuracy_D":0.8854166667,
    "avg_word_count":239.4675,
    "accuracy":0.8275,
    "RSD":0.0390115486,
    "# of none":12,
    "FR":0.15,
    "FR_2":0.02
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8369565217,
    "accuracy_B":0.8383838384,
    "accuracy_C":0.83,
    "accuracy_D":0.8446601942,
    "avg_word_count":236.435,
    "accuracy":0.825,
    "RSD":0.006221425,
    "# of none":6,
    "FR":0.1525,
    "FR_2":0.03
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8571428571,
    "accuracy_B":0.8571428571,
    "accuracy_C":0.8350515464,
    "accuracy_D":0.85,
    "avg_word_count":180.62,
    "accuracy":0.85,
    "RSD":0.0106129477,
    "# of none":0,
    "FR":0.14,
    "FR_2":0.0025
  },
  {
    "subtask":"human_sexuality",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8105263158,
    "accuracy_B":0.8736842105,
    "accuracy_C":0.820754717,
    "accuracy_D":0.8076923077,
    "avg_word_count":177.27,
    "accuracy":0.8275,
    "RSD":0.0322716465,
    "# of none":0,
    "FR":0.155,
    "FR_2":0.0125
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8787878788,
    "accuracy_B":0.9175257732,
    "accuracy_C":0.8811881188,
    "accuracy_D":0.8737864078,
    "avg_word_count":152.89,
    "accuracy":0.8875,
    "RSD":0.0195490663,
    "# of none":0,
    "FR":0.0625,
    "FR_2":0.0
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8256880734,
    "accuracy_B":0.8969072165,
    "accuracy_C":0.9042553191,
    "accuracy_D":0.8979591837,
    "avg_word_count":236.385,
    "accuracy":0.875,
    "RSD":0.0365116655,
    "# of none":2,
    "FR":0.11,
    "FR_2":0.015
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.8440366972,
    "accuracy_B":0.90625,
    "accuracy_C":0.8888888889,
    "accuracy_D":0.9368421053,
    "avg_word_count":158.815,
    "accuracy":0.89,
    "RSD":0.0375509157,
    "# of none":1,
    "FR":0.11,
    "FR_2":0.0025
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.887755102,
    "accuracy_B":0.8910891089,
    "accuracy_C":0.8571428571,
    "accuracy_D":0.8958333333,
    "avg_word_count":108.7075,
    "accuracy":0.8825,
    "RSD":0.0171884738,
    "# of none":0,
    "FR":0.075,
    "FR_2":0.015
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8775510204,
    "accuracy_B":0.8571428571,
    "accuracy_C":0.8888888889,
    "accuracy_D":0.8865979381,
    "avg_word_count":119.7975,
    "accuracy":0.875,
    "RSD":0.0142657676,
    "# of none":1,
    "FR":0.0925,
    "FR_2":0.01
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9072164948,
    "accuracy_B":0.9166666667,
    "accuracy_C":0.8679245283,
    "accuracy_D":0.8811881188,
    "avg_word_count":165.7625,
    "accuracy":0.8925,
    "RSD":0.0218968563,
    "# of none":0,
    "FR":0.095,
    "FR_2":0.01
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.858490566,
    "accuracy_B":0.8673469388,
    "accuracy_C":0.898989899,
    "accuracy_D":0.9255319149,
    "avg_word_count":273.3325,
    "accuracy":0.88,
    "RSD":0.0299468983,
    "# of none":3,
    "FR":0.0875,
    "FR_2":0.0075
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.87,
    "accuracy_B":0.8888888889,
    "accuracy_C":0.8476190476,
    "accuracy_D":0.9333333333,
    "avg_word_count":254.9725,
    "accuracy":0.87,
    "RSD":0.0356152754,
    "# of none":6,
    "FR":0.1225,
    "FR_2":0.0075
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8725490196,
    "accuracy_B":0.8834951456,
    "accuracy_C":0.8969072165,
    "accuracy_D":0.9081632653,
    "avg_word_count":116.5075,
    "accuracy":0.89,
    "RSD":0.0151132975,
    "# of none":0,
    "FR":0.08,
    "FR_2":0.0025
  },
  {
    "subtask":"international_law",
    "model":"gemini-2.0-flash",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9090909091,
    "accuracy_B":0.89,
    "accuracy_C":0.8834951456,
    "accuracy_D":0.8979591837,
    "avg_word_count":103.6625,
    "accuracy":0.895,
    "RSD":0.0106656635,
    "# of none":0,
    "FR":0.0875,
    "FR_2":0.0
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9625,
    "accuracy_B":0.898989899,
    "accuracy_C":0.8230088496,
    "accuracy_D":0.8425925926,
    "avg_word_count":173.2725,
    "accuracy":0.875,
    "RSD":0.0616004282,
    "# of none":0,
    "FR":0.1525,
    "FR_2":0.0075
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.9512195122,
    "accuracy_B":0.8490566038,
    "accuracy_C":0.8198198198,
    "accuracy_D":0.90625,
    "avg_word_count":240.26,
    "accuracy":0.865,
    "RSD":0.0576451591,
    "# of none":5,
    "FR":0.1725,
    "FR_2":0.0025
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9638554217,
    "accuracy_B":0.8775510204,
    "accuracy_C":0.8173913043,
    "accuracy_D":0.854368932,
    "avg_word_count":171.3475,
    "accuracy":0.87,
    "RSD":0.0613216305,
    "# of none":1,
    "FR":0.1525,
    "FR_2":0.0125
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9518072289,
    "accuracy_B":0.8865979381,
    "accuracy_C":0.8392857143,
    "accuracy_D":0.8518518519,
    "avg_word_count":190.945,
    "accuracy":0.8775,
    "RSD":0.0494867449,
    "# of none":0,
    "FR":0.1625,
    "FR_2":0.0075
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"en",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.8965517241,
    "accuracy_B":0.88,
    "accuracy_C":0.8348623853,
    "accuracy_D":0.8461538462,
    "avg_word_count":187.3775,
    "accuracy":0.8625,
    "RSD":0.0288205768,
    "# of none":0,
    "FR":0.1375,
    "FR_2":0.015
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9347826087,
    "accuracy_B":0.8823529412,
    "accuracy_C":0.8846153846,
    "accuracy_D":0.8725490196,
    "avg_word_count":185.02,
    "accuracy":0.8925,
    "RSD":0.0271042304,
    "# of none":0,
    "FR":0.12,
    "FR_2":0.0075
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"json",
    "total_exp":400,
    "accuracy_A":0.8854166667,
    "accuracy_B":0.8725490196,
    "accuracy_C":0.9191919192,
    "accuracy_D":0.8775510204,
    "avg_word_count":280.7175,
    "accuracy":0.8775,
    "RSD":0.0204855134,
    "# of none":5,
    "FR":0.1025,
    "FR_2":0.015
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"base",
    "output_format":"xml",
    "total_exp":400,
    "accuracy_A":0.9638554217,
    "accuracy_B":0.8834951456,
    "accuracy_C":0.8518518519,
    "accuracy_D":0.8679245283,
    "avg_word_count":285.52,
    "accuracy":0.8875,
    "RSD":0.048318521,
    "# of none":0,
    "FR":0.1425,
    "FR_2":0.01
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"json",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9010989011,
    "accuracy_B":0.8775510204,
    "accuracy_C":0.8363636364,
    "accuracy_D":0.8712871287,
    "avg_word_count":226.765,
    "accuracy":0.87,
    "RSD":0.0265834393,
    "# of none":0,
    "FR":0.1125,
    "FR_2":0.005
  },
  {
    "subtask":"international_law",
    "model":"mistral-small-latest",
    "language":"fr",
    "input_format":"xml",
    "output_format":"base",
    "total_exp":400,
    "accuracy_A":0.9431818182,
    "accuracy_B":0.858490566,
    "accuracy_C":0.8703703704,
    "accuracy_D":0.887755102,
    "avg_word_count":218.7225,
    "accuracy":0.8875,
    "RSD":0.0364605453,
    "# of none":0,
    "FR":0.09,
    "FR_2":0.0125
  }
]